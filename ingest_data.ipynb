{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing and building standardization statistics for pretraining NF_UNSW_NB15 dataset...\n",
      "- Loading in all training set file nr 0...\n",
      "--- Dropping 5802 rows with NaNs before preprocessing:\n",
      "-- Unique values for one-hot encoding columns not found. Finding these values and writing to file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\Documenten\\02-work\\01-BNN-UPC\\PPT_GNN_github_version\\data_handling\\data_preprocessing.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ohe[f'{attribute}_{value}'] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Min-Max scaling ranges not found. Calculating these ranges and writing to file...\n",
      "-- Min-Max scaling numerical columns...\n",
      "Testing preprocessing and building standardization statistics for pretraining NF_ToN_IoT dataset...\n",
      "- Loading in all training set file nr 0...\n",
      "-- Min-Max scaling numerical columns...\n",
      "Testing preprocessing and building standardization statistics for pretraining NF_UNSW_NB15 dataset...\n",
      "- Loading in all training set file nr 0...\n",
      "--- Dropping 5446 rows with NaNs before preprocessing:\n",
      "-- Min-Max scaling numerical columns...\n"
     ]
    }
   ],
   "source": [
    "from data_handling.data_ingestion import DataIngestion\n",
    "from data_handling.data_preprocessing import DataPreprocessor\n",
    "import os\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "DATASETS_TO_INGEST = ['NF_UNSW_NB15', 'NF_ToN_IoT', 'NF_UNSW_NB15'] #, 'NF_ToN_IoT', 'NF_UNSW_NB15']\n",
    "TEST_PREPROCESSING_AND_BUILD_STANDARDIZATION_STATISTICS = True\n",
    "\n",
    "# Define the directories for the different stages of the data pipeline\n",
    "raw_data_dir = f'{DATA_FOLDER}/raw'\n",
    "landed_data_dir = f'{DATA_FOLDER}/landed'\n",
    "ingested_data_dir = f'{DATA_FOLDER}/ingested'\n",
    "utils_data_dir = f'{DATA_FOLDER}/utils'\n",
    "\n",
    "# Create the data ingestion object\n",
    "data_ingestor = DataIngestion(raw_data_dir, landed_data_dir, ingested_data_dir, utils_data_dir)\n",
    "\n",
    "print('Starting data ingestion...')\n",
    "\n",
    "# Land the datasets if not already landed\n",
    "for dataset in DATASETS_TO_INGEST:\n",
    "    print(f'Landing {dataset} dataset...')\n",
    "    if dataset == 'NF_ToN_IoT':\n",
    "        data_ingestor.land_raw_NF_ToN_IoT_files()\n",
    "    elif dataset == 'NF_BoT_IoT':\n",
    "        data_ingestor.land_raw_NF_BoT_IoT_files()\n",
    "    elif dataset == 'NF_UNSW_NB15':\n",
    "        data_ingestor.land_raw_NF_UNSW_NB15_files()\n",
    "    else:\n",
    "        print(f'Error: Landing and ingestion routine not yet developed for dataset {dataset}.')\n",
    "# Ingest the datasets if not already ingested\n",
    "for dataset in DATASETS_TO_INGEST:\n",
    "    print(f'Ingesting {dataset} dataset...')\n",
    "    data_ingestor.ingest_attack_files(dataset) # Ingest binary and mixed files for supervised learning\n",
    "    data_ingestor.ingest_all_train_files(dataset) # Ingest all training files for unsupervised learning\n",
    "\n",
    "print('Data ingestion complete.')\n",
    "\n",
    "if TEST_PREPROCESSING_AND_BUILD_STANDARDIZATION_STATISTICS:\n",
    "    print('Testing preprocessing and building standardization statistics...')\n",
    "\n",
    "    # Create the data preprocessor object\n",
    "    data_processor = DataPreprocessor(ingested_data_dir, utils_data_dir)\n",
    "\n",
    "    for dataset_name in DATASETS_TO_INGEST:\n",
    "        print(f'- for supervised {dataset_name} dataset...')\n",
    "        attack_mapping = data_processor.load_attack_mapping(dataset_name)\n",
    "        train_raw, val_raw, = data_processor.load_mixed_train(dataset_name), data_processor.load_mixed_val(dataset_name)\n",
    "        (train_attrs, train_labels), (val_attrs, val_labels) = data_processor.preprocess_NF(dataset_name, train_raw, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True), \\\n",
    "                            data_processor.preprocess_NF(dataset_name, val_raw, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True)\n",
    "    \n",
    "    for dataset_name in DATASETS_TO_INGEST:\n",
    "        print(f'- for pretraining {dataset_name} dataset...')\n",
    "        attack_mapping = data_processor.load_attack_mapping(dataset_name)\n",
    "        all_train_files_and_indices = data_processor.get_all_train_files_and_indices('NF_ToN_IoT')\n",
    "        # Take first file for testing\n",
    "        all_train_file, all_train_file_idx = all_train_files_and_indices[0]\n",
    "        dataset = data_processor.load_all_train(dataset_name, all_train_file_idx, 0.1)\n",
    "        preprocessed_train_attrs, labels = data_processor.preprocess_NF('all', dataset, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
