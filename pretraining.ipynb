{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handling\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from util_scripts import gnn_architectures\n",
    "from util_scripts.link_prediction_pretraining import train_model, load_link_prediction_model, process_data_and_build_graphs, get_metadata_and_sample_graph, process_data_and_build_out_of_context_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "current_dir = os.getcwd()\n",
    "DATA_PATH = f'{current_dir}/data/ingested'\n",
    "UTILS_PATH = f'{current_dir}/data/utils'\n",
    "SAVED_MODELS_PATH = f'{current_dir}/saved_models'\n",
    "CONFIG_PATH = f'{current_dir}/configs'\n",
    "SAVED_GRAPHS_PATH = f'{current_dir}/data/saved_graphs'\n",
    "\n",
    "# General hyperparameters\n",
    "dataset_name = 'NF_ToN_IoT' # Choose from ['NF_ToN_IoT', 'NF_UNSW_NB15', 'NF_BoT_IoT', 'all']\n",
    "truncate = True\n",
    "graph_type = 'temporal' # Choose from ['static', 'temporal']\n",
    "graph_building = 'normal'\n",
    "pre_training_strategy = 'out_context' # Choose from ['in_context', 'out_context']\n",
    "\n",
    "# Model hyperparameters\n",
    "gnn_layers = 2 # 2,3\n",
    "gnn_hidden_channels = 128\n",
    "window_size = 5 #20, 10, 5, 1, 0.5 # 2, 5, 10, 20 # 10, 30\n",
    "graph_building = 'normal' # 'connect_flows', 'normal'\n",
    "include_port = False # True, False\n",
    "self_loops = False\n",
    "window_memory = 5 # [3, 5]\n",
    "classifier_layers = 2\n",
    "classifier_hidden_channels = 128\n",
    "\n",
    "# Specific hyperparameters for temporal graph + model type\n",
    "flow_memory = 20\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 3\n",
    "batch_size = 4 # Keep low enough to not run out of memory (especially with BoT_IoT involved)\n",
    "save_epoch_every = 1\n",
    "learning_rate = 0.001\n",
    "continue_training_from_checkpoint = False\n",
    "\n",
    "# Specific for out_context pre-training\n",
    "use_last_saved_graphs = True # Use the last saved graphs for out_context pre-training if they exist (check hyperparameters in that file to see if they fit, otherwise set to False)\n",
    "\n",
    "# For if continue_training_from_checkpoint is True\n",
    "checkpoint_dir = '/content/drive/MyDrive/BNN-UPC/pre_training/saved_models/all/pretraining_experiments/TemporalPlus_v2/experiment_5' # Set directory of checkpoint if continue_training_from_checkpoint is True\n",
    "checkpoint_epoch = 20 # Set epoch of checkpoint if continue_training_from_checkpoint is True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading in all training set file nr 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\Documenten\\02-work\\01-BNN-UPC\\PPT_GNN_github_version\\data_handling\\data_preprocessing.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ohe[f'{attribute}_{value}'] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Min-Max scaling numerical columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on unlabeled training sets that are not the dataset itself (the target dataset on which we'll later fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [02:31<05:02, 151.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- Training Loss: 288.2467613220215, Validation Loss: 288.10518860816956, validation Acc: 0.5150745370262246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [03:56<01:52, 112.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Training Loss: 286.9778376221657, Validation Loss: 286.7381873726845, validation Acc: 0.5304013743675012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:29<00:00, 109.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 -- Training Loss: 281.8711902499199, Validation Loss: 281.4733741879463, validation Acc: 0.5768741382111748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_preprocessor = data_handling.DataPreprocessor(DATA_PATH, UTILS_PATH)\n",
    "graph_builder = data_handling.GraphBuilder()\n",
    "\n",
    "# If training should continue from a checkpoint, override some parts of the config with the checkpoint config\n",
    "if continue_training_from_checkpoint:\n",
    "\n",
    "    # set experiemnt dir to the checkpoint dir\n",
    "    experiment_dir = checkpoint_dir\n",
    "\n",
    "# Else create a new experiment dir\n",
    "else:\n",
    "    os.makedirs(os.path.join(SAVED_MODELS_PATH, dataset_name, 'pretraining_experiments', graph_type), exist_ok=True)\n",
    "    model_dir = os.path.join(SAVED_MODELS_PATH, dataset_name, 'pretraining_experiments', graph_type)\n",
    "    experiment_idx = len(os.listdir(model_dir))\n",
    "    experiment_dir = os.path.join(model_dir, f'experiment_{experiment_idx}')\n",
    "    os.makedirs(experiment_dir)\n",
    "\n",
    "# Build initial graph list for graph sample and metadata\n",
    "if pre_training_strategy != 'in_context':\n",
    "    # Just get dummy attack mapping and sample graph. Doesnt matter. is for initalization and is standardized over all datasets.\n",
    "    attack_mapping = data_preprocessor.load_attack_mapping('NF_ToN_IoT')\n",
    "    all_train_files_and_indices = data_preprocessor.get_all_train_files_and_indices('NF_ToN_IoT')\n",
    "    graph_metadata, graph_sample, features = get_metadata_and_sample_graph(True, 'NF_ToN_IoT', data_preprocessor, graph_builder, all_train_files_and_indices[0][1], graph_type, window_size, window_memory, include_port, attack_mapping)\n",
    "else:\n",
    "    attack_mapping = data_preprocessor.load_attack_mapping(dataset_name)\n",
    "    all_train_files_and_indices = data_preprocessor.get_all_train_files_and_indices(dataset_name)\n",
    "    graph_metadata, graph_sample, features = get_metadata_and_sample_graph(False, dataset_name, data_preprocessor, graph_builder, all_train_files_and_indices[0][1], graph_type, window_size, window_memory, include_port, attack_mapping)\n",
    "\n",
    "# Initialize model\n",
    "starting_epoch = 0\n",
    "if graph_type == 'temporal':\n",
    "    gnn_base = gnn_architectures.TemporalPlusConv_v2(graph_metadata, gnn_hidden_channels, gnn_layers)\n",
    "elif graph_type == 'static':\n",
    "    gnn_base = gnn_architectures.SAGE(graph_metadata, gnn_hidden_channels, gnn_layers)\n",
    "else:\n",
    "    raise ValueError('Unknown GNN type')\n",
    "\n",
    "model = gnn_architectures.LinkPredictionModel(gnn_base)\n",
    "\n",
    "if continue_training_from_checkpoint: # Load model from checkpoint if needed\n",
    "    model = load_link_prediction_model(model, checkpoint_dir, checkpoint_epoch, graph_metadata, graph_type, graph_sample)\n",
    "    starting_epoch = checkpoint_epoch + 1\n",
    "\n",
    "if pre_training_strategy == 'in_context':\n",
    "    print('Training on curated training set')\n",
    "\n",
    "    # Process data and build graphs\n",
    "    train_graphs = process_data_and_build_graphs(False, dataset_name, data_preprocessor, graph_builder, graph_type, window_size, window_memory, include_port, attack_mapping, flow_memory, on_curated_train=True)\n",
    "\n",
    "    # Save the experiment metadata\n",
    "    experiments_metadata = {'pre_training_strategy': pre_training_strategy ,'graph_type': graph_type, 'flow_memory': flow_memory, 'gnn_layer': gnn_layers, 'window_size': window_size, 'window_memory': window_memory, 'include_port': include_port, 'self_loops': self_loops, 'gnn_hidden_channels': gnn_hidden_channels, 'classifier_layers': classifier_layers, 'classifier_hidden_channels': classifier_hidden_channels}\n",
    "    with open(os.path.join(experiment_dir, 'experiment_metadata.json'), 'w') as f:\n",
    "        json.dump(experiments_metadata, f)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_graphs, graph_metadata, graph_type, learning_rate, batch_size, experiment_dir, save_epoch_every, starting_epoch=starting_epoch, epoch_end=num_epochs)\n",
    "\n",
    "elif pre_training_strategy == 'out_context':\n",
    "    print(\"Training on unlabeled training sets that are not the dataset itself (the target dataset on which we'll later fine-tune)\")\n",
    "\n",
    "    current_epoch = starting_epoch\n",
    "\n",
    "    datasets = ['NF_BoT_IoT', 'NF_ToN_IoT', 'NF_UNSW_NB15']\n",
    "    datasets.remove(dataset_name)\n",
    "    if use_last_saved_graphs and os.path.exists(os.path.join(SAVED_GRAPHS_PATH, f'{datasets[0]}_{datasets[1]}_link_prediction_training_graph_list.pt')):\n",
    "        mixed_graphs = torch.load(os.path.join(SAVED_GRAPHS_PATH, f'{datasets[0]}_{datasets[1]}_link_prediction_training_graph_list.pt'))\n",
    "    else:\n",
    "        print(f'Saved graph list of mixed datasets {datasets[0]} and {datasets[1]} not found! Building new graph list and saving them...')\n",
    "        mixed_graphs = process_data_and_build_out_of_context_graphs(datasets, data_preprocessor, graph_builder, graph_type, window_size, window_memory, include_port, flow_memory, idx=0)\n",
    "        torch.save(mixed_graphs, os.path.join(SAVED_GRAPHS_PATH, f'{datasets[0]}_{datasets[1]}_link_prediction_training_graph_list.pt'))     \n",
    "        print(f'Graph list of mixed datasets {datasets[0]} and {datasets[1]} built and saved at {os.path.join(SAVED_GRAPHS_PATH, f\"{datasets[0]}_{datasets[1]}_link_prediction_training_graph_list.pt\")}')  \n",
    "        print(f\"Saving hyperparameters and metadata of the saved pretrain graphs at {os.path.join(SAVED_GRAPHS_PATH, f'{datasets[0]}_{datasets[1]}_link_prediction_training_graph_list_metadata.json')}\")\n",
    "        with open(os.path.join(SAVED_GRAPHS_PATH, f'{datasets[0]}_{datasets[1]}_link_prediction_training_graph_list_metadata.json'), 'w') as f:\n",
    "            json.dump({'graph_type': graph_type, 'window_size': window_size, 'window_memory': window_memory, 'include_port': include_port, 'flow_memory': flow_memory}, f)\n",
    "\n",
    "    # Save the experiment metadata\n",
    "    experiments_metadata = {'pre_training_strategy': pre_training_strategy ,'graph_type': graph_type, 'flow_memory': flow_memory, 'gnn_layer': gnn_layers, 'window_size': window_size, 'window_memory': window_memory, 'include_port': include_port, 'self_loops': self_loops, 'gnn_hidden_channels': gnn_hidden_channels, 'classifier_layers': classifier_layers, 'classifier_hidden_channels': classifier_hidden_channels}\n",
    "    with open(os.path.join(experiment_dir, 'experiment_metadata.json'), 'w') as f:\n",
    "        json.dump(experiments_metadata, f)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, mixed_graphs, graph_metadata, graph_type, learning_rate, batch_size, experiment_dir, save_epoch_every, starting_epoch=starting_epoch, epoch_end=num_epochs)\n",
    "\n",
    "else:\n",
    "    print('No valid pretraining strategy specified. Choose from \"in_context\", \"out_context\" or \"mixed_context\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
