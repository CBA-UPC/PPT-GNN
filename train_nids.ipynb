{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handling\n",
    "from torch_geometric.transforms import AddSelfLoops\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from util_scripts import gnn_architectures\n",
    "from util_scripts.gnn_training import train_batch, validate_batch, calculate_multiclass_metrics, calculate_multiclass_test_metrics, export_pretty_confusion_matrix, deduplicate_multiclass_sliding_window_results, balanced_temporal_undersampler\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: General GNNs from scratch Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "RAW_DATA_PATH = 'data/raw'\n",
    "LANDED_DATA_PATH = 'data/landed'\n",
    "INGESTED_DATA_PATH = 'data/ingested'\n",
    "UTILS_PATH = 'data/utils'\n",
    "SAVED_MODELS_PATH = 'saved_models'\n",
    "CONFIG_PATH = 'configs'\n",
    "\n",
    "# General parameters\n",
    "dataset_name = 'NF_ToN_IoT' # Pick from 'NF_ToN_IoT', 'NF_BoT_IoT' and 'NF_UNSW_NB15'\n",
    "truncate = True\n",
    "gnn_type = 'temporal' # Pick from 'temporal' and 'static'\n",
    "temporal = True if gnn_type == 'temporal' else False\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 10\n",
    "num_epochs = 2\n",
    "weighted_loss = True\n",
    "\n",
    "# Model Parameters. Make list of all the options you want to try\n",
    "gnn_layer_options = [2] # [2,3]\n",
    "window_size_options = [10, 1] # [10, 30]\n",
    "self_loops_options = [False]\n",
    "save_epoch_every = 1\n",
    "\n",
    "# Specific for if temporal\n",
    "window_memory_options = [3, 5] # [3, 5]\n",
    "flow_memory = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading in mixed training set...\n",
      "- Loading in mixed validation set...\n",
      "-- Min-Max scaling numerical columns...\n",
      "-- Min-Max scaling numerical columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2390/2390 [01:48<00:00, 22.08it/s]\n",
      "100%|██████████| 586/586 [00:33<00:00, 17.30it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[168716   7729      0  13249     88   1627   5093   4889      0      0]\n",
      " [  1011  24780      0      1     20      1    286      3      0      0]\n",
      " [ 19561   3355      0   2564    184      0    120   2716      0      0]\n",
      " [  8510    529      0  52605      0    316   1528     93      0      0]\n",
      " [ 12635    273      0  19625      0   1267  44407  15050      0      0]\n",
      " [   556    121      0    378      0    512    136    412      0      0]\n",
      " [ 38571    985      0  19517    303    196  31220   9626      0      0]\n",
      " [  2379    296      0    100      0    101    124   1244      0      0]\n",
      " [ 14411      0      0      0      0      0      0      0      0      0]\n",
      " [  6605      0      0    818      0      0   3369    872      0      0]]\n",
      "Epoch: 0 Train Loss: 463.7584934234619 Train Accuracy: 0.5114269640065753 Train Multiclass Weighted F1: 0.43325596069296\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:26<00:26, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41844     7     0  2775     0  2858  8479  2294     0     0]\n",
      " [   43  7716     0     0     0     4     0     1     0     0]\n",
      " [    6     0     0    14     0     4  8599    26     0     0]\n",
      " [    2     0     0 25337     0     5     0     0     0     0]\n",
      " [    1     0     0   428     0  1601 33722  2621     0     0]\n",
      " [   13     0     0     3     0   423     0    99     0     0]\n",
      " [    0     0     0     8     0    64 38843   380     0     0]\n",
      " [   40     0     0     0     0    13     5   782     0     0]\n",
      " [    4     0     0    35     0     0  7771     0     0     0]\n",
      " [   63     0     0   536     0    27  2164  1753     0     0]]\n",
      "Validation Loss: 94.26711678504944 Validation Accuracy: 0.6005078025003526 Validation Multiclass Weighted F1: 0.5336303263300253 Validation Multiclass Macro F1: 0.3651171142762194\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[177189     83      0   2532      1   5109  13537   2940      0      0]\n",
      " [   141  25934      0      2      0     12      0     13      0      0]\n",
      " [ 13493      0      0      3      0    173  14831      0      0      0]\n",
      " [   151      0      0  63291      0    139      0      0      0      0]\n",
      " [  2675      3      0     82   3827  19616  65486   1568      0      0]\n",
      " [   161      0      0    214      0   1646     49     45      0      0]\n",
      " [   918      0      0    581    103    626  95320   2870      0      0]\n",
      " [   121      0      0      0      0    179    155   3789      0      0]\n",
      " [     0      1      0      3      0      1  14406      0      0      0]\n",
      " [  1693      0      0    367      0     33   9209    362      0      0]]\n",
      "Epoch: 1 Train Loss: 391.0983818769455 Train Accuracy: 0.6798745791970796 Train Multiclass Weighted F1: 0.6186987913980861\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:51<00:00, 25.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42519    52     0  1578  2293  4249  6166  1400     0     0]\n",
      " [   30  7714     0     1     0    17     0     2     0     0]\n",
      " [ 1499     0     0     2  5174     2  1972     0     0     0]\n",
      " [    0     0     0 25343     0     1     0     0     0     0]\n",
      " [    0     0     0    10 29240   948  7756   419     0     0]\n",
      " [    1     0     0     0     0   525     0    12     0     0]\n",
      " [    0     0     0     0  6384    24 32865    22     0     0]\n",
      " [   10     0     0     0     0    24     4   802     0     0]\n",
      " [   78     0     0     9   217     7  7499     0     0     0]\n",
      " [  559     0     0   303  1528  1415   691    47     0     0]]\n",
      "Validation Loss: 92.14862620830536 Validation Accuracy: 0.7262202671709863 Validation Multiclass Weighted F1: 0.7027551308313615 Validation Multiclass Macro F1: 0.4757027702919466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1434/1434 [01:39<00:00, 14.40it/s]\n",
      "100%|██████████| 352/352 [00:29<00:00, 11.78it/s]\n",
      "  0%|          | 0/2 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m     epoch_targets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((epoch_targets, batch_targets))\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m epoch_train_accuracy, epoch_train_f1_weighted, epoch_train_f1_macro \u001b[38;5;241m=\u001b[39m calculate_multiclass_metrics(epoch_preds, epoch_targets, attack_mapping)\n\u001b[0;32m    107\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(total_train_loss)\n\u001b[0;32m    108\u001b[0m train_weighted_f1\u001b[38;5;241m.\u001b[39mappend(epoch_train_f1_weighted)\n",
      "Cell \u001b[1;32mIn[6], line 49\u001b[0m, in \u001b[0;36mcalculate_multiclass_metrics\u001b[1;34m(out, target, attack_mapping)\u001b[0m\n\u001b[0;32m     47\u001b[0m acc \u001b[38;5;241m=\u001b[39m (out \u001b[38;5;241m==\u001b[39m target)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(target)\n\u001b[0;32m     48\u001b[0m f1_weighted \u001b[38;5;241m=\u001b[39m f1_score(target, out, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m f1_macro \u001b[38;5;241m=\u001b[39m f1_score(target, out, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(attack_mapping\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(target, out))\n",
      "File \u001b[1;32mc:\\Users\\louis\\anaconda3\\envs\\pytorchEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1_score\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     y_true,\n\u001b[0;32m   1013\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m ):\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fbeta_score(\n\u001b[0;32m   1147\u001b[0m         y_true,\n\u001b[0;32m   1148\u001b[0m         y_pred,\n\u001b[0;32m   1149\u001b[0m         beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1150\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1151\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m   1152\u001b[0m         average\u001b[38;5;241m=\u001b[39maverage,\n\u001b[0;32m   1153\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1154\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[0;32m   1155\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\louis\\anaconda3\\envs\\pytorchEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfbeta_score\u001b[39m(\n\u001b[0;32m   1159\u001b[0m     y_true,\n\u001b[0;32m   1160\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m ):\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \n\u001b[0;32m   1171\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1288\u001b[0m         y_true,\n\u001b[0;32m   1289\u001b[0m         y_pred,\n\u001b[0;32m   1290\u001b[0m         beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[0;32m   1291\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1292\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m   1293\u001b[0m         average\u001b[38;5;241m=\u001b[39maverage,\n\u001b[0;32m   1294\u001b[0m         warn_for\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-score\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[0;32m   1295\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1296\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[0;32m   1297\u001b[0m     )\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\louis\\anaconda3\\envs\\pytorchEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1577\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1577\u001b[0m MCM \u001b[38;5;241m=\u001b[39m multilabel_confusion_matrix(\n\u001b[0;32m   1578\u001b[0m     y_true,\n\u001b[0;32m   1579\u001b[0m     y_pred,\n\u001b[0;32m   1580\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1581\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1582\u001b[0m     samplewise\u001b[38;5;241m=\u001b[39msamplewise,\n\u001b[0;32m   1583\u001b[0m )\n\u001b[0;32m   1584\u001b[0m tp_sum \u001b[38;5;241m=\u001b[39m MCM[:, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1585\u001b[0m pred_sum \u001b[38;5;241m=\u001b[39m tp_sum \u001b[38;5;241m+\u001b[39m MCM[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\louis\\anaconda3\\envs\\pytorchEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:497\u001b[0m, in \u001b[0;36mmultilabel_confusion_matrix\u001b[1;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n\u001b[1;32m--> 497\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    499\u001b[0m     labels \u001b[38;5;241m=\u001b[39m present_labels\n",
      "File \u001b[1;32mc:\\Users\\louis\\anaconda3\\envs\\pytorchEnv\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:81\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo argument has been passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Check that we don't mix label format\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m ys_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(type_of_target(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ys)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ys_types \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m     83\u001b[0m     ys_types \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\louis\\anaconda3\\envs\\pytorchEnv\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:81\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo argument has been passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Check that we don't mix label format\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m ys_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(type_of_target(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ys)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ys_types \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m     83\u001b[0m     ys_types \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\louis\\anaconda3\\envs\\pytorchEnv\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:386\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n\u001b[0;32m    385\u001b[0m first_row \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\u001b[38;5;241m.\u001b[39mgetrow(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m--> 386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39munique_values(y)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first_row) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;66;03m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\louis\\anaconda3\\envs\\pytorchEnv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:84\u001b[0m, in \u001b[0;36m_NumPyApiWrapper.unique_values\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39munique(x)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\louis\\anaconda3\\envs\\pytorchEnv\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:276\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                     equal_nan\u001b[38;5;241m=\u001b[39mequal_nan)\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\louis\\anaconda3\\envs\\pytorchEnv\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:125\u001b[0m, in \u001b[0;36m_unpack_tuple\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    121\u001b[0m     np\u001b[38;5;241m.\u001b[39msubtract(ary[\u001b[38;5;241m1\u001b[39m:], ary[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], result[l_begin:l_begin \u001b[38;5;241m+\u001b[39m l_diff])\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unpack_tuple\u001b[39m(x):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Unpacks one-element tuples for use as return values \"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_processor = data_handling.DataPreprocessor(INGESTED_DATA_PATH, UTILS_PATH)\n",
    "graph_builder = data_handling.GraphBuilder()\n",
    "\n",
    "attack_mapping = data_processor.load_attack_mapping(dataset_name)\n",
    "train_raw, val_raw, = data_processor.load_mixed_train(dataset_name), data_processor.load_mixed_val(dataset_name)\n",
    "(train_attrs, train_labels), (val_attrs, val_labels) = data_processor.preprocess_NF(dataset_name, train_raw, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=truncate), \\\n",
    "                    data_processor.preprocess_NF(dataset_name, val_raw, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=truncate)\n",
    "\n",
    "# If this dataset, we undersampxle benign flows a bit for performance. Only on training set though, not on eval!\n",
    "if dataset_name == 'NF_UNSW_NB15':\n",
    "    train_attrs, train_labels = data_processor.randomly_drop_benign_flows(train_attrs, train_labels, 0.7)\n",
    "\n",
    "for gnn_layers in gnn_layer_options:\n",
    "    gnn_hidden_channels = 128\n",
    "    classifier_layers = 2\n",
    "    classifier_hidden_channels = 128\n",
    "\n",
    "    for window_size in window_size_options:\n",
    "        for window_memory in window_memory_options:\n",
    "            window_stride = window_size\n",
    "            # batch_size = max(1,int((1/(window_size*window_memory))*200)+10) # A metric for having small enough batch size when windows are getting big\n",
    "            for self_loops in self_loops_options:\n",
    "\n",
    "                features = train_attrs.columns\n",
    "                features = [feat for feat in features if feat not in ['Dst IP', 'Dst Port', 'Flow Duration Graph Building', 'Src IP', 'Src Port', 'Timestamp']]\n",
    "                train_windows, val_windows = graph_builder.time_window_with_flow_duration(train_attrs, window_size, window_stride), \\\n",
    "                                                            graph_builder.time_window_with_flow_duration(val_attrs, window_size, window_stride)\n",
    "                if gnn_type == 'temporal':\n",
    "                    train_graphs, _ = graph_builder.build_spatio_temporal_pyg_graphs(train_windows, train_attrs, train_labels, window_memory, flow_memory, False, features, attack_mapping, True)\n",
    "                    val_graphs, val_window_indices_for_classification = graph_builder.build_spatio_temporal_pyg_graphs(val_windows, val_attrs, val_labels, window_memory, flow_memory, False, features, attack_mapping, True)\n",
    "                elif gnn_type == 'static':\n",
    "                    train_graphs = graph_builder.build_static_pyg_graphs(train_windows, train_attrs, train_labels, False, features, attack_mapping, True)\n",
    "                    val_graphs = graph_builder.build_static_pyg_graphs(val_windows, val_attrs, val_labels, False, features, attack_mapping, True)\n",
    "\n",
    "                metadata = train_graphs[0].metadata()\n",
    "                sample_graph = train_graphs[0]\n",
    "\n",
    "                if gnn_type == 'temporal':\n",
    "                    gnn_base = gnn_architectures.TemporalPlusConv_v2(metadata, gnn_hidden_channels, gnn_layers)\n",
    "                elif gnn_type == 'static':\n",
    "                    gnn_base = gnn_architectures.SAGE(metadata, gnn_hidden_channels, gnn_layers)\n",
    "                else:\n",
    "                    raise ValueError('Unknown GNN type')\n",
    "\n",
    "                model = gnn_architectures.multiclass_NIDS_model(gnn_base, len(attack_mapping), classifier_hidden_channels, classifier_layers, temporal)\n",
    "\n",
    "                os.makedirs(os.path.join(SAVED_MODELS_PATH, dataset_name, 'experiments', gnn_type), exist_ok=True)\n",
    "                model_dir = os.path.join(SAVED_MODELS_PATH, dataset_name, 'experiments', gnn_type)\n",
    "                experiment_idx = len(os.listdir(model_dir))\n",
    "                experiment_dir = os.path.join(model_dir, f'experiment_{experiment_idx}')\n",
    "                os.makedirs(experiment_dir)\n",
    "                experiment_dict = {'dataset': dataset_name, 'gnn_type': gnn_type, 'gnn_layers': gnn_layers, 'gnn_hidden_channels': gnn_hidden_channels, 'classifier_layers': classifier_layers, 'classifier_hidden_channels': classifier_hidden_channels, 'self_loops': self_loops, 'window_size': window_size, 'window_stride': window_stride, 'include_port': False, 'window_memory': window_memory, 'batch_size': batch_size, 'num_epochs': num_epochs, 'weighted_loss': weighted_loss, 'truncate': truncate, 'flow_memory': flow_memory}\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                train_weighted_f1 = []\n",
    "                val_weighted_f1 = []\n",
    "                train_macro_f1 = []\n",
    "                val_macro_f1 = []\n",
    "\n",
    "                # Setup optimizer\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "                loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "                loader_val = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                # Set model to device\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                model.to(device)\n",
    "\n",
    "                # Setup Loss Criterion\n",
    "                if weighted_loss:\n",
    "                    num_classes = len(attack_mapping)\n",
    "                    class_counts = np.zeros(num_classes)\n",
    "                    for batch in loader:\n",
    "                        target = batch['con'].y\n",
    "                        class_counts += target.sum(dim=0).cpu().numpy()\n",
    "\n",
    "                    # print(f'class counts in training data: {attack_mapping.keys()}:{class_counts}')\n",
    "                    total_samples = class_counts.sum()\n",
    "                    class_weights = total_samples / (num_classes * class_counts)\n",
    "                    weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "                    criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "                else:\n",
    "                    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "                # Train the model\n",
    "                best_model_weights_macro = None\n",
    "                best_model_weights_weighted = None\n",
    "                best_model_val_f1_macro = 0\n",
    "                best_model_val_f1_weighted = 0\n",
    "                best_model_weights_macro_epoch = 0\n",
    "                best_model_weights_weighted_epoch = 0\n",
    "\n",
    "                for epoch in tqdm(range(num_epochs)):\n",
    "                    total_train_loss = 0\n",
    "                    epoch_preds = np.array([])\n",
    "                    epoch_targets = np.array([])\n",
    "                    for batch in loader:\n",
    "                        train_data = batch.to(device)\n",
    "                        batch_loss, batch_preds, batch_targets = train_batch(model, train_data, optimizer, criterion)\n",
    "                        total_train_loss += batch_loss\n",
    "                        epoch_preds = np.concatenate((epoch_preds, batch_preds))\n",
    "                        epoch_targets = np.concatenate((epoch_targets, batch_targets))\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    epoch_train_accuracy, epoch_train_f1_weighted, epoch_train_f1_macro = calculate_multiclass_metrics(epoch_preds, epoch_targets, attack_mapping)\n",
    "                    train_losses.append(total_train_loss)\n",
    "                    train_weighted_f1.append(epoch_train_f1_weighted)\n",
    "                    train_macro_f1.append(epoch_train_f1_macro)\n",
    "                    print('Epoch:', epoch, 'Train Loss:', total_train_loss, 'Train Accuracy:', epoch_train_accuracy.item(), 'Train Multiclass Weighted F1:', epoch_train_f1_weighted.item())\n",
    "\n",
    "                    total_val_loss = 0\n",
    "                    epoch_preds = np.array([])\n",
    "                    epoch_targets = np.array([])\n",
    "                    for batch in loader_val:\n",
    "                        val_data = batch.to(device)\n",
    "                        batch_loss, batch_preds, batch_targets = validate_batch(model, val_data, criterion)\n",
    "                        total_val_loss += batch_loss\n",
    "                        epoch_preds = np.concatenate((epoch_preds, batch_preds))\n",
    "                        epoch_targets = np.concatenate((epoch_targets, batch_targets))\n",
    "\n",
    "                    val_accuracy, val_f1_weighted, val_f1_macro = calculate_multiclass_metrics(epoch_preds, epoch_targets, attack_mapping)\n",
    "                    val_losses.append(total_val_loss)\n",
    "                    val_weighted_f1.append(val_f1_weighted)\n",
    "                    val_macro_f1.append(val_f1_macro)\n",
    "                    if val_f1_macro > best_model_val_f1_macro:\n",
    "                        best_model_val_f1_macro = val_f1_macro\n",
    "                        best_model_weights_macro = model.state_dict()\n",
    "                        best_model_weights_macro_epoch = epoch\n",
    "                    if val_f1_weighted > best_model_val_f1_weighted:\n",
    "                        best_model_val_f1_weighted = val_f1_weighted\n",
    "                        best_model_weights_weighted = model.state_dict()\n",
    "                        best_model_weights_weighted_epoch = epoch\n",
    "                    if (epoch % save_epoch_every == 0) and (epoch != 0):\n",
    "                        torch.save(model.state_dict, os.path.join(experiment_dir, f'model_weights_checkpoint_epoch_{epoch}.pth'))\n",
    "\n",
    "                    print('Validation Loss:', total_val_loss, 'Validation Accuracy:', val_accuracy.item(), 'Validation Multiclass Weighted F1:', val_f1_weighted.item(), 'Validation Multiclass Macro F1:', val_f1_macro.item())\n",
    "\n",
    "                # Save the best models\n",
    "                torch.save(best_model_weights_macro, os.path.join(experiment_dir, f'best_model_weights_macro_f1_epoch_{best_model_weights_macro_epoch}.pth'))\n",
    "                torch.save(best_model_weights_weighted, os.path.join(experiment_dir, f'best_model_weights_weighted_f1_epoch_{best_model_weights_weighted_epoch}.pth'))\n",
    "\n",
    "                # Save the experiment metadata\n",
    "                experiments_results = {'train_losses': train_losses, 'val_losses': val_losses, 'train_weighted_f1': train_weighted_f1, 'val_weighted_f1': val_weighted_f1, 'train_macro_f1': train_macro_f1, 'val_macro_f1': val_macro_f1}\n",
    "                with open(os.path.join(experiment_dir, 'results.json'), 'w') as f:\n",
    "                    json.dump(experiments_results, f)\n",
    "                with open(os.path.join(experiment_dir, 'experiment_metadata.json'), 'w') as f:\n",
    "                    json.dump(experiment_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: GNNs from pre-trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "RAW_DATA_PATH = 'data/raw'\n",
    "LANDED_DATA_PATH = 'data/landed'\n",
    "INGESTED_DATA_PATH = 'data/ingested'\n",
    "UTILS_PATH = 'data/utils'\n",
    "SAVED_MODELS_PATH = 'saved_models'\n",
    "CONFIG_PATH = 'configs'\n",
    "\n",
    "# General parameters\n",
    "dataset_name = 'NF_ToN_IoT' # 'NF_UNSW_NB15', 'NF_ToN_IoT', 'NF_BoT_IoT'\n",
    "pretraining_strategy = 'no_pretraining' # 'in_context','out_context','no_pretraining'\n",
    "undersample_fracs = [0.05] # [0.1, 0.2, 0.5, 0.8, 1.0] # K-shot learning\n",
    "\n",
    "# Pretrained model parameters (which one to select)\n",
    "experiment_idx = 0\n",
    "checkpoint_idx = 2\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 10\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "weighted_loss = True\n",
    "classifier_layers = 2\n",
    "classifier_hidden_channels = 128\n",
    "truncate = True # Truncate the extreme numerical values in standardization\n",
    "\n",
    "# For if pretraining_strategy=='no_pretraining'. Else, ignore (we'll load the best model from the pretraining)\n",
    "flow_memory = 20\n",
    "window_size = 5\n",
    "window_memory = 5\n",
    "window_stride = 5\n",
    "use_ports=False\n",
    "self_loops=False\n",
    "gnn_hidden_channels = 128\n",
    "gnn_type = 'temporal' # 'temporal', 'static'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading in mixed validation set...\n",
      "- Loading in mixed test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\Documenten\\02-work\\01-BNN-UPC\\PPT_GNN_github_version\\data_handling\\data_preprocessing.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ohe[f'{attribute}_{value}'] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Min-Max scaling numerical columns...\n",
      "-- Min-Max scaling numerical columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 694/694 [00:22<00:00, 31.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading in mixed training set...\n",
      "-- Min-Max scaling numerical columns...\n",
      "Original Dataset Subsampled in balanced temporal way to 0.04997435531577669 % of the original dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:03<00:00, 35.69it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[   0    0  965    0    0 1557 6272    0    0    0]\n",
      " [   0    0   66    0    0  255 1069    0    0    0]\n",
      " [   0    0    0    0    0   41 2447    0    0    0]\n",
      " [   0    0  302    0    0  951  580    0    0    0]\n",
      " [   0    0    0    0    0    0 2763    0    0    0]\n",
      " [   0    0  112    0    0   74  769    0    0    0]\n",
      " [   0    0 1477    0    0    1 1121    0    0    0]\n",
      " [   0    0  112    0    0  254 1192    0    0    0]\n",
      " [   0    0    0    0    0    0 1298    0    0    0]\n",
      " [   0    0    0    0    0   62 2646    0    0    0]]\n",
      "Epoch: 0 Train Loss: 32.24578642845154 Train Accuracy: 0.04528916849844614 Train Multiclass Weighted F1: 0.010995236880890913 Train Multiclass Macro F1: 0.01341861169409577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:02<00:11,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[    0     0     0     0     0 61700     0     0     0     0]\n",
      " [    0     0     0     0     0 10364     0     0     0     0]\n",
      " [    0     0     0     0     0  8721     0     0     0     0]\n",
      " [    0     0     0     0     0 25716     0     0     0     0]\n",
      " [    0     0     0     0     0 39319     0     0     0     0]\n",
      " [    0     0     0     0     0   769     0     0     0     0]\n",
      " [    0     0     0     0     0 40186     0     0     0     0]\n",
      " [    0     0     0     0     0   879     0     0     0     0]\n",
      " [    0     0     0     0     0 10175     0     0     0     0]\n",
      " [    0     0     0     0     0  4789     0     0     0     0]]\n",
      "Validation Loss: 160.66599893569946 Validation Accuracy: 0.0037953192707459358 Validation Multiclass Weighted F1: 2.8699971180101235e-05 Validation Multiclass Macro F1: 0.0007561938570311771\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[ 115 3737    0    0    0 4942    0    0    0    0]\n",
      " [   0  939    0    0    0  451    0    0    0    0]\n",
      " [   2 2069    0    0    0  417    0    0    0    0]\n",
      " [   0   20    0    0    0 1813    0    0    0    0]\n",
      " [   0  203    0    0    0 2560    0    0    0    0]\n",
      " [  24  533    0    0    0  398    0    0    0    0]\n",
      " [   0 1110    0    0    0 1489    0    0    0    0]\n",
      " [  12  725    0    0    0  821    0    0    0    0]\n",
      " [   0    0    0    0    0 1298    0    0    0    0]\n",
      " [   0    3    0    0    0 2705    0    0    0    0]]\n",
      "Epoch: 1 Train Loss: 32.1562135219574 Train Accuracy: 0.055029182142045024 Train Multiclass Weighted F1: 0.019402766615179826 Train Multiclass Macro F1: 0.02453428890690413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:05<00:08,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[ 2053 59212     0     0     0   435     0     0     0     0]\n",
      " [    1 10363     0     0     0     0     0     0     0     0]\n",
      " [    2  8719     0     0     0     0     0     0     0     0]\n",
      " [  978 24191     0     0     0   547     0     0     0     0]\n",
      " [  532 25004     0     0     0 13783     0     0     0     0]\n",
      " [    8   744     0     0     0    17     0     0     0     0]\n",
      " [  267 25965     0     0     0 13954     0     0     0     0]\n",
      " [   73   727     0     0     0    79     0     0     0     0]\n",
      " [   13 10162     0     0     0     0     0     0     0     0]\n",
      " [  598  2347     0     0     0  1844     0     0     0     0]]\n",
      "Validation Loss: 159.4394612312317 Validation Accuracy: 0.06136177437345152 Validation Multiclass Weighted F1: 0.024846830372379048 Validation Multiclass Macro F1: 0.017965308296023517\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[3411 5118    0    0    0  265    0    0    0    0]\n",
      " [   0 1390    0    0    0    0    0    0    0    0]\n",
      " [2024  464    0    0    0    0    0    0    0    0]\n",
      " [ 778  813    0    0    0  242    0    0    0    0]\n",
      " [ 199   24    0    0    0 2540    0    0    0    0]\n",
      " [ 312  576    0    0    0   67    0    0    0    0]\n",
      " [   0 2100    0    0    0  499    0    0    0    0]\n",
      " [ 754  616    0    0    0  174    0   14    0    0]\n",
      " [   0 1298    0    0    0    0    0    0    0    0]\n",
      " [ 306   30    0    0    0 2372    0    0    0    0]]\n",
      "Epoch: 2 Train Loss: 31.941834688186646 Train Accuracy: 0.18502236034260594 Train Multiclass Weighted F1: 0.14948005631326652 Train Multiclass Macro F1: 0.0649329330777277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:09<00:06,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[30693 30923     0     0     0    14     0    70     0     0]\n",
      " [    3 10361     0     0     0     0     0     0     0     0]\n",
      " [ 8607   114     0     0     0     0     0     0     0     0]\n",
      " [22047  3669     0     0     0     0     0     0     0     0]\n",
      " [31285   424     0     0     0    30     0  7580     0     0]\n",
      " [  289   480     0     0     0     0     0     0     0     0]\n",
      " [25161   879     0     0     0    54     0 14092     0     0]\n",
      " [  665   141     0     0     0     0     0    73     0     0]\n",
      " [10081    94     0     0     0     0     0     0     0     0]\n",
      " [ 3291    41     0     0     0     0     0  1457     0     0]]\n",
      "Validation Loss: 155.204097032547 Validation Accuracy: 0.20297801774768284 Validation Multiclass Weighted F1: 0.11490675950399622 Validation Multiclass Macro F1: 0.06832038661682807\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[4786 3929    0    0    0   18    0   61    0    0]\n",
      " [   1 1389    0    0    0    0    0    0    0    0]\n",
      " [ 500   79    0    0    0    0    0 1909    0    0]\n",
      " [1547  222    0    0    0   57    0    7    0    0]\n",
      " [  35    8    0    0    0    0    0 2720    0    0]\n",
      " [ 627  238    0    0    0   61    0   29    0    0]\n",
      " [ 165   31    0    0    0    0    0 2403    0    0]\n",
      " [ 509   63    0    0    0   20    0  966    0    0]\n",
      " [1275   23    0    0    0    0    0    0    0    0]\n",
      " [1467   10    0    0    0    0    0 1231    0    0]]\n",
      "Epoch: 3 Train Loss: 31.250651359558105 Train Accuracy: 0.27294777533540515 Train Multiclass Weighted F1: 0.1961689603725381 Train Multiclass Macro F1: 0.11493804140327016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:12<00:03,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[38935 21798     0     0     0     0     0   967     0     0]\n",
      " [    6 10358     0     0     0     0     0     0     0     0]\n",
      " [ 8301   188     0     0     0     0     0   232     0     0]\n",
      " [21111  1807     0     0     0     0     0  2798     0     0]\n",
      " [ 7050   218     0     0     0     0     0 32051     0     0]\n",
      " [  456   306     0     0     0     0     0     7     0     0]\n",
      " [ 1469   807     0     0     0     0     0 37910     0     0]\n",
      " [  113    99     0     0     0     0     0   667     0     0]\n",
      " [ 9933   111     0     0     0     0     0   131     0     0]\n",
      " [  897    18     0     0     0     0     0  3874     0     0]]\n",
      "Validation Loss: 144.52745473384857 Validation Accuracy: 0.24657236770671906 Validation Multiclass Weighted F1: 0.18118516355517894 Validation Multiclass Macro F1: 0.0985634733411892\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[5932 2113    0    0    0    0    0  749    0    0]\n",
      " [   2 1388    0    0    0    0    0    0    0    0]\n",
      " [ 314   26    0    0    0    0    0 2148    0    0]\n",
      " [1501   73    0   72    0    0    0  187    0    0]\n",
      " [ 147   18    0    0    0    0    0 2598    0    0]\n",
      " [ 898   33    0    0    0    2    0   22    0    0]\n",
      " [  69   47    0    0    0    0    0 2483    0    0]\n",
      " [ 187   46    0    0    0    0    0 1325    0    0]\n",
      " [1273   22    0    0    0    0    0    3    0    0]\n",
      " [ 358   12    0    0    0    0    0 2338    0    0]]\n",
      "Epoch: 4 Train Loss: 30.517658591270447 Train Accuracy: 0.3304403850526795 Train Multiclass Weighted F1: 0.24839980458266334 Train Multiclass Macro F1: 0.14237122390523077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:14<00:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[50130  2810     0     4     0   210     0  8546     0     0]\n",
      " [   49 10315     0     0     0     0     0     0     0     0]\n",
      " [  864    20     0     0     0     0     0  7837     0     0]\n",
      " [22952   157     0  1905     0   190     0   512     0     0]\n",
      " [ 4763    15     0     2     0    66     0 34473     0     0]\n",
      " [  574    46     0     0     0    82     0    67     0     0]\n",
      " [  610   198     0     0     0   219     0 39159     0     0]\n",
      " [  104    18     0     0     0     0     0   757     0     0]\n",
      " [ 3461    28     0     0     0     0     0  6686     0     0]\n",
      " [  402     0     0     0     0     0     0  4387     0     0]]\n",
      "Validation Loss: 130.96599411964417 Validation Accuracy: 0.31186271703402463 Validation Multiclass Weighted F1: 0.2716682208559149 Validation Multiclass Macro F1: 0.18085149084970759\n",
      "Execution time: 14.97993016242981 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_processor = data_handling.DataPreprocessor(INGESTED_DATA_PATH, UTILS_PATH)\n",
    "graph_builder = data_handling.GraphBuilder()\n",
    "\n",
    "if pretraining_strategy != 'no_pretraining':\n",
    "  pre_trained_gnn_dir = f'{SAVED_MODELS_PATH}/{dataset_name}/pretraining_experiments/{gnn_type}/experiment_{experiment_idx}'\n",
    "  weights_path = f'{pre_trained_gnn_dir}/checkpoint_{checkpoint_idx}_gnnbase.pt'\n",
    "  metadata_path = f'{pre_trained_gnn_dir}/experiment_metadata.json'\n",
    "  with open(metadata_path, 'r') as f:\n",
    "      metadata = json.load(f)\n",
    "\n",
    "  gnn_type, flow_memory, gnn_layers, window_size, window_memory, include_port, self_loops, gnn_hidden_channels, classifier_layers, classifier_hidden_channels = metadata[\"graph_type\"],metadata[\"flow_memory\"],metadata[\"gnn_layer\"],metadata[\"window_size\"],metadata[\"window_memory\"],metadata[\"include_port\"],metadata[\"self_loops\"],metadata[\"gnn_hidden_channels\"], metadata[\"classifier_layers\"], metadata[\"classifier_hidden_channels\"]\n",
    "  window_stride = window_size\n",
    "  temporal = True if gnn_type == 'temporal' else False\n",
    "\n",
    "attack_mapping = data_processor.load_attack_mapping(dataset_name)\n",
    "truncate = True\n",
    "val_raw, test_raw = data_processor.load_mixed_val(dataset_name), data_processor.load_mixed_test(dataset_name)\n",
    "\n",
    "if pretraining_strategy != 'in_context':\n",
    "  (val_attrs, val_labels), (test_attrs, test_labels) = data_processor.preprocess_NF('all', val_raw, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True), \\\n",
    "                            data_processor.preprocess_NF('all', test_raw, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True)\n",
    "\n",
    "else:\n",
    "  (val_attrs, val_labels), (test_attrs, test_labels) = data_processor.preprocess_NF(dataset_name, val_raw, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True), \\\n",
    "                            data_processor.preprocess_NF(dataset_name, test_raw, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True)\n",
    "\n",
    "features = val_attrs.columns\n",
    "features = [feat for feat in features if feat not in ['Dst IP', 'Dst Port', 'Flow Duration Graph Building', 'Src IP', 'Src Port', 'Timestamp']]\n",
    "val_windows = graph_builder.time_window_with_flow_duration(val_attrs, window_size, window_stride)\n",
    "if gnn_type == 'temporal':\n",
    "    val_graphs, val_window_indices_for_classification = graph_builder.build_spatio_temporal_pyg_graphs(val_windows, val_attrs, val_labels, window_memory, flow_memory, include_port, features, attack_mapping, True)\n",
    "elif gnn_type == 'static':\n",
    "    val_graphs = graph_builder.build_static_pyg_graphs(val_windows, val_attrs, val_labels, include_port, features, attack_mapping, True)\n",
    "else:\n",
    "    raise ValueError('GNN type not recognized!')\n",
    "if self_loops:\n",
    "    val_graph = [AddSelfLoops()(graph) for graph in val_graphs]\n",
    "\n",
    "# Undersample the training data for K-shot learning\n",
    "os.makedirs(os.path.join(SAVED_MODELS_PATH, dataset_name, 'fine_tuned_experiments', gnn_type), exist_ok=True)\n",
    "experiment_idx = len(os.listdir(os.path.join(SAVED_MODELS_PATH, dataset_name, 'fine_tuned_experiments', gnn_type)))\n",
    "\n",
    "for undersample_frac in undersample_fracs:\n",
    "    train_raw = data_processor.load_mixed_train(dataset_name)\n",
    "\n",
    "    if pretraining_strategy != 'in_context':\n",
    "        train_attrs, train_labels = data_processor.preprocess_NF('all', train_raw, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True)\n",
    "\n",
    "    else:\n",
    "       train_attrs, train_labels= data_processor.preprocess_NF(dataset_name, train_raw, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True)\n",
    "\n",
    "    train_attrs, train_labels, practical_fraction = balanced_temporal_undersampler(train_attrs, train_labels, undersample_frac)\n",
    "\n",
    "    features = train_attrs.columns\n",
    "    features = [feat for feat in features if feat not in ['Dst IP', 'Dst Port', 'Flow Duration Graph Building', 'Src IP', 'Src Port', 'Timestamp']]\n",
    "\n",
    "    train_windows = graph_builder.time_window_with_flow_duration(train_attrs, window_size, window_stride)\n",
    "\n",
    "    if gnn_type == 'temporal':\n",
    "        train_graphs, _ = graph_builder.build_spatio_temporal_pyg_graphs(train_windows, train_attrs, train_labels, window_memory, flow_memory, include_port, features, attack_mapping, True)\n",
    "    elif gnn_type == 'static':\n",
    "        train_graphs = graph_builder.build_static_pyg_graphs(train_windows, train_attrs, train_labels, include_port, features, attack_mapping, True)\n",
    "    else:\n",
    "        raise ValueError('GNN type not recognized!')\n",
    "\n",
    "    if self_loops:\n",
    "        train_graph = [AddSelfLoops()(graph) for graph in train_graphs]\n",
    "\n",
    "    metadata = train_graphs[0].metadata()\n",
    "    sample_graph = train_graphs[0]\n",
    "\n",
    "    if gnn_type == 'temporal':\n",
    "        gnn_base = gnn_architectures.TemporalPlusConv_v2(metadata, gnn_hidden_channels, gnn_layers)\n",
    "    elif gnn_type == 'static':\n",
    "        gnn_base = gnn_architectures.SAGE(metadata, gnn_hidden_channels, gnn_layers)\n",
    "    else:\n",
    "        raise ValueError('Unknown GNN type')\n",
    "\n",
    "    if pretraining_strategy != 'no_pretraining':\n",
    "      # Load the pre-trained model\n",
    "      with torch.no_grad():  # Initialize lazy modules.\n",
    "          out = gnn_base(sample_graph.x_dict, sample_graph.edge_index_dict)\n",
    "      gnn_base.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))\n",
    "      print(f'Pretrained GNN loaded successfully!')\n",
    "\n",
    "    model = gnn_architectures.multiclass_NIDS_model(gnn_base, len(attack_mapping), classifier_hidden_channels, classifier_layers)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_weighted_f1 = []\n",
    "    val_weighted_f1 = []\n",
    "    train_macro_f1 = []\n",
    "    val_macro_f1 = []\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Setup dataloader\n",
    "    loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "    loader_val = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Setup Loss Criterion\n",
    "    if weighted_loss:\n",
    "        num_classes = len(attack_mapping)\n",
    "        class_counts = np.zeros(num_classes)\n",
    "        for batch in loader:\n",
    "            target = batch['con'].y\n",
    "            class_counts += target.sum(dim=0).cpu().numpy()\n",
    "\n",
    "        # print(f'class counts in training data: {attack_mapping.keys()}:{class_counts}')\n",
    "        total_samples = class_counts.sum()\n",
    "        class_weights = total_samples / (num_classes * class_counts)\n",
    "        # print(f'Class Weights: {attack_mapping.keys()}')\n",
    "        # print(class_weights)\n",
    "        weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Train the model\n",
    "    best_model_weights = None\n",
    "    best_model_val_f1 = 0\n",
    "    best_model_weights_epoch = 0\n",
    "\n",
    "    # get the start time\n",
    "    st = time.time()\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_train_loss = 0\n",
    "        epoch_preds = np.array([])\n",
    "        epoch_targets = np.array([])\n",
    "        for batch in loader:\n",
    "            train_data = batch.to(device)\n",
    "            batch_loss, batch_preds, batch_targets = train_batch(model, train_data, optimizer, criterion)\n",
    "            total_train_loss += batch_loss\n",
    "            epoch_preds = np.concatenate((epoch_preds, batch_preds))\n",
    "            epoch_targets = np.concatenate((epoch_targets, batch_targets))\n",
    "\n",
    "        # Calculate metrics\n",
    "        epoch_train_accuracy, epoch_train_f1_weighted, epoch_train_f1_macro = calculate_multiclass_metrics(epoch_preds, epoch_targets, attack_mapping)\n",
    "        train_losses.append(total_train_loss)\n",
    "        train_weighted_f1.append(epoch_train_f1_weighted)\n",
    "        train_macro_f1.append(epoch_train_f1_macro)\n",
    "        print('Epoch:', epoch, 'Train Loss:', total_train_loss, 'Train Accuracy:', epoch_train_accuracy.item(), 'Train Multiclass Weighted F1:', epoch_train_f1_weighted.item(), 'Train Multiclass Macro F1:', epoch_train_f1_macro.item())\n",
    "\n",
    "        total_val_loss = 0\n",
    "        epoch_preds = np.array([])\n",
    "        epoch_targets = np.array([])\n",
    "        for batch in loader_val:\n",
    "            val_data = batch.to(device)\n",
    "            batch_loss, batch_preds, batch_targets = validate_batch(model, val_data, criterion)\n",
    "            total_val_loss += batch_loss\n",
    "            epoch_preds = np.concatenate((epoch_preds, batch_preds))\n",
    "            epoch_targets = np.concatenate((epoch_targets, batch_targets))\n",
    "\n",
    "        val_accuracy, val_f1_weighted, val_f1_macro = calculate_multiclass_metrics(epoch_preds, epoch_targets, attack_mapping)\n",
    "        val_losses.append(total_val_loss)\n",
    "        val_weighted_f1.append(val_f1_weighted)\n",
    "        val_macro_f1.append(val_f1_macro)\n",
    "        if val_f1_macro > best_model_val_f1:\n",
    "            best_model_val_f1 = val_f1_macro\n",
    "            best_model_weights = model.state_dict()\n",
    "            best_model_weights_epoch = epoch\n",
    "        print('Validation Loss:', total_val_loss, 'Validation Accuracy:', val_accuracy.item(), 'Validation Multiclass Weighted F1:', val_f1_weighted.item(), 'Validation Multiclass Macro F1:', val_f1_macro.item())\n",
    "\n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "    # get the execution time\n",
    "    elapsed_time = et - st\n",
    "    print('Execution time:', elapsed_time, 'seconds')\n",
    "\n",
    "    experiment_dir = os.path.join(SAVED_MODELS_PATH, dataset_name, 'fine_tuned_experiments', gnn_type, f'experiment_{experiment_idx}', str(undersample_frac))\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "    experiment_dict = {'dataset': dataset_name, 'K-shot-dataset_frac': practical_fraction, 'pretrain_strategy': pretraining_strategy, 'gnn_type': gnn_type, 'gnn_layers': gnn_layers, 'gnn_hidden_channels': gnn_hidden_channels, 'classifier_layers': classifier_layers, 'classifier_hidden_channels': classifier_hidden_channels, 'self_loops': self_loops, 'window_size': window_size, 'window_stride': window_stride, 'window_memory': window_memory, 'batch_size': batch_size, 'num_epochs': num_epochs, 'execution_time': elapsed_time}\n",
    "\n",
    "    # Save the best model\n",
    "    torch.save(best_model_weights, os.path.join(experiment_dir, f'best_model_weights_epoch_{best_model_weights_epoch}.pth'))\n",
    "\n",
    "    # Save the experiment metadata\n",
    "    experiments_results = {'train_losses': train_losses, 'val_losses': val_losses, 'train_weighted_f1': train_weighted_f1, 'val_weighted_f1': val_weighted_f1, 'train_macro_f1': train_macro_f1, 'val_macro_f1': val_macro_f1}\n",
    "    with open(os.path.join(experiment_dir, 'results.json'), 'w') as f:\n",
    "        json.dump(experiments_results, f)\n",
    "    with open(os.path.join(experiment_dir, 'experiment_metadata.json'), 'w') as f:\n",
    "        json.dump(experiment_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Reference MLP Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "RAW_DATA_PATH = 'data/raw'\n",
    "LANDED_DATA_PATH = 'data/landed'\n",
    "INGESTED_DATA_PATH = 'data/ingested'\n",
    "UTILS_PATH = 'data/utils'\n",
    "SAVED_MODELS_PATH = 'saved_models'\n",
    "CONFIG_PATH = 'configs'\n",
    "\n",
    "# General Experiment parameters\n",
    "dataset_name = 'NF_ToN_IoT' # 'NF_ToN_IoT', 'NF_BoT_IoT' or 'NF_UNSW_NB15'\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "weighted_loss = True\n",
    "classifier_layers = 2\n",
    "classifier_hidden_channels = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading in mixed training set...\n",
      "- Loading in mixed validation set...\n",
      "- Loading in mixed test set...\n",
      "-- Min-Max scaling numerical columns...\n",
      "-- Min-Max scaling numerical columns...\n",
      "-- Min-Max scaling numerical columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[63076    13  8611  2459  3921  7472  5113  2735   248  1493]\n",
      " [    7  8581    32     2     2    23     2    22     2    12]\n",
      " [   90     5 11376   269   135   208   185   132    29   190]\n",
      " [  261    11   177 29009     3  1181     4    27    10   643]\n",
      " [  299     2  1801   163 37266   467  2217   577    56  1943]\n",
      " [   54     2    16    26    18   481    16    16     0    32]\n",
      " [  549    55   839    56  3196   963 34911   825  2034   861]\n",
      " [   29     1    29     0    12    43    37  1690     4    30]\n",
      " [    4     0    14     0     3    10   288    37 14047     7]\n",
      " [   25     0    22   172   100   216    31   137     2  4811]]\n",
      "Epoch: 0 Train Loss: 936.7169901132584 Train Accuracy: 0.7915067890927181 Train Multiclass Weighted F1: 0.8126846699606932 Train Multiclass Macro F1: 0.6999639039766464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:13<02:05, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[21628     8   312   162   735  1852  2155   208    87    33]\n",
      " [    1  2560    11     1     1     6     2     2     0     1]\n",
      " [  285     0  3693     0   251     3    55     0     0     1]\n",
      " [   29     1     3 12303     0   145     1     0     4     2]\n",
      " [   14     0   185   168 15589  1209  1151     0     2   354]\n",
      " [    5     0     4     1     3   111    14     2     0     1]\n",
      " [    2     0   150     0  5635    10 13378     0     0     1]\n",
      " [    2     0    18     0     0    12    16   354     0     0]\n",
      " [    4     0     0     0     0     3    63    11  5512     0]\n",
      " [    2     0     6    10   215     4    29    31     0  1939]]\n",
      "Validation Loss: 359.02946863044053 Validation Accuracy: 0.8308125181919126 Validation Multiclass Weighted F1: 0.8458514326090888 Validation Multiclass Macro F1: 0.7803253074687739\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[76020    27  4505  1002  1411  6525  2793  2027   194   637]\n",
      " [   12  8603    33     0     0    22     2     9     1     3]\n",
      " [  338     0 11781   115   152    86   105    16     0    26]\n",
      " [  203     8    82 30503     0   489     0     0     3    38]\n",
      " [  343     1   813   101 40343   345  1300   108    36  1401]\n",
      " [   50     0    12     7    12   545     6    11     1    17]\n",
      " [  528     0   331    11  1165   925 39953   307   295   774]\n",
      " [   15     0     6     0     4    39    27  1781     1     2]\n",
      " [    0     0     2     0     0     7    60     6 14335     0]\n",
      " [   22     0    10     3    48   113    28    23     1  5268]]\n",
      "Epoch: 1 Train Loss: 548.2756193242967 Train Accuracy: 0.8836116970610807 Train Multiclass Weighted F1: 0.8990172786535391 Train Multiclass Macro F1: 0.7880279033689954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:27<01:51, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[20308    10  3163    85   980  1302   597   599   102    34]\n",
      " [    2  2560    12     1     0     3     2     4     0     1]\n",
      " [   15     0  3922     0   317     2    30     1     0     1]\n",
      " [   21     3     0 12291     0   156     0     2    13     2]\n",
      " [  747     0   113   168 15003  1002   448     8     2  1181]\n",
      " [   13     0     0     1     7   103     1    15     0     1]\n",
      " [ 2795     0     6     0  2749  2514  8470     1     0  2641]\n",
      " [    1     0     4     0     0     0    20   377     0     0]\n",
      " [    4     0     0     0     0     0    59    14  5516     0]\n",
      " [   34     0     6    10   204     4     5    29     0  1944]]\n",
      "Validation Loss: 355.70869114995 Validation Accuracy: 0.7599529974881685 Validation Multiclass Weighted F1: 0.7781100743975269 Validation Multiclass Macro F1: 0.6853978938858795\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[79370    46  3439   767   992  5462  2269  1897   175   724]\n",
      " [   16  8608    24     0     0    33     0     1     1     2]\n",
      " [  419     0 11789    81   143    91    76     8     0    12]\n",
      " [  160     3   133 30605     1   408     0     2     3    11]\n",
      " [  338     2   737   105 40668   260  1264    69    21  1327]\n",
      " [   37     1     8     8     6   571     4    10     1    15]\n",
      " [  396     1   271    10  1007   677 40803   235   137   752]\n",
      " [   14     1     3     0     3    37    23  1792     1     1]\n",
      " [    0     0     1     0     2     9    54     8 14336     0]\n",
      " [   38     0     7     4    39   105    26    10     2  5285]]\n",
      "Epoch: 2 Train Loss: 451.02678550966084 Train Accuracy: 0.9017172297570889 Train Multiclass Weighted F1: 0.9145759814964117 Train Multiclass Macro F1: 0.8031267161863678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:40<01:34, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[25045     7   287   153   128   548   609   164   202    37]\n",
      " [    6  2560     6     1     0     6     0     2     2     2]\n",
      " [  285     0  3715     0   259     2    26     0     0     1]\n",
      " [   97     0     3 12305     0    77     0     0     1     5]\n",
      " [  683     0   134   168 13325  1395  2319     1     2   645]\n",
      " [   11     0     4     2     3   104    14     1     0     2]\n",
      " [ 1097     0   150     0  5580    88 12166     0    48    47]\n",
      " [   33     0    16     0     0     1    14   338     0     0]\n",
      " [   12     0     0     0     0     3    67     3  5508     0]\n",
      " [   63     0     6    10    69     3    12     2     0  2071]]\n",
      "Validation Loss: 384.6580617837608 Validation Accuracy: 0.8315671456754455 Validation Multiclass Weighted F1: 0.8375500827415227 Validation Multiclass Macro F1: 0.7782221926299646\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[80744    30  3017   661   923  5061  2116  1750   161   678]\n",
      " [   18  8606    13     0     0    38     0     8     0     2]\n",
      " [  399     0 11854    49   135    67    88    13     1    13]\n",
      " [  172     2    62 30638     2   416     0     2     2    30]\n",
      " [  317     0   795   101 40838   249  1116    65    20  1290]\n",
      " [   33     1     9     8     4   577     6    13     1     9]\n",
      " [  404     3   275     6   965   647 40952   195   127   715]\n",
      " [   12     0     4     0     5    34    14  1804     0     2]\n",
      " [    0     0     0     0     4    13    43     7 14343     0]\n",
      " [   37     0    12    11    38    70    16    14     3  5315]]\n",
      "Epoch: 3 Train Loss: 423.2999635171145 Train Accuracy: 0.9088283271567565 Train Multiclass Weighted F1: 0.9207050046562455 Train Multiclass Macro F1: 0.8107956989064349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:55<01:23, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[24608    10   161   125   746   590   606   149   142    43]\n",
      " [   12  2560     1     1     0     6     0     2     2     1]\n",
      " [  239     0  3700     0   318     2    27     0     1     1]\n",
      " [   97     0     0 12307     0    79     0     0     1     4]\n",
      " [  526     0    99   168 13910  1088  1976     3     2   900]\n",
      " [    9     0     1     1     7   108    13     1     0     1]\n",
      " [   26     0     2     0  5416    10 13685     0     0    37]\n",
      " [    6     0    14     0     0    11     6   365     0     0]\n",
      " [    8     0     0     0     0     9    59     1  5516     0]\n",
      " [   32     0    13     2   107     2    18    19     0  2043]]\n",
      "Validation Loss: 339.562127432524 Validation Accuracy: 0.8495164993909078 Validation Multiclass Weighted F1: 0.8578154353079428 Validation Multiclass Macro F1: 0.7917252954356615\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[80968    41  2895   615   963  5119  2026  1665   160   689]\n",
      " [   26  8607    10     0     0    32     0     8     0     2]\n",
      " [  405     1 11857    58   130    71    81     8     0     8]\n",
      " [  158     0    69 30668     5   371     1     1     1    52]\n",
      " [  338     1   733    91 40813   328  1075    78    22  1312]\n",
      " [   27     0     9     8     7   591     2     6     1    10]\n",
      " [  376     0   250     9   977   601 41005   176   185   710]\n",
      " [   13     0     3     0     7    26    14  1811     1     0]\n",
      " [   12     0     1     0     2     3    70     9 14312     1]\n",
      " [   42     0     5     1    40    80    28    11     0  5309]]\n",
      "Epoch: 4 Train Loss: 414.91881713829935 Train Accuracy: 0.9098695398996579 Train Multiclass Weighted F1: 0.921666232138972 Train Multiclass Macro F1: 0.8127317678961967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:09<01:09, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[21567     9  2633    23    72   781  1731   232    94    38]\n",
      " [    1  2560     0     1     0    18     2     2     0     1]\n",
      " [   86     0  3923     0   245     2    29     2     0     1]\n",
      " [  104     0     5 11820     0   556     2     0     1     0]\n",
      " [  269     0   115     4 13482  3185   859    45     1   712]\n",
      " [    7     0     1     0     7   111     8     7     0     0]\n",
      " [    3     0   150     0  5302    15 13705     0     0     1]\n",
      " [   14     0    14     0     0     4     2   368     0     0]\n",
      " [   17     0     0     0     0     0    67     1  5508     0]\n",
      " [  473     0    15     1   149     5    27    39     0  1527]]\n",
      "Validation Loss: 418.94554751052056 Validation Accuracy: 0.8039046582076519 Validation Multiclass Weighted F1: 0.8260191368513293 Validation Multiclass Macro F1: 0.7393502480542192\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[80963    36  2948   436  1059  5104  2143  1633   139   680]\n",
      " [   19  8607     7     1     0    39     0    10     0     2]\n",
      " [  407     0 11870    32   151    76    66    10     1     6]\n",
      " [  136     2    23 30652     3   492     1     2     1    14]\n",
      " [  333     2   761    90 40994   300  1046    53    13  1199]\n",
      " [   29     0    11     5     6   587     7     8     1     7]\n",
      " [  375     0   265     3   917   655 41147   143   126   658]\n",
      " [   10     1     3     0     4    28     7  1821     0     1]\n",
      " [    1     0     0     0     1    17    38     6 14346     1]\n",
      " [   41     0     6     1    34    69    19     9     2  5335]]\n",
      "Epoch: 5 Train Loss: 400.90310316719115 Train Accuracy: 0.9113388067701966 Train Multiclass Weighted F1: 0.9232426782554285 Train Multiclass Macro F1: 0.8160886199253241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:22<00:54, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[25612    11   175    97    86   438   406   219    94    42]\n",
      " [   13  2560     1     1     0     4     3     2     0     1]\n",
      " [  282     0  3698     0   277     3    27     0     0     1]\n",
      " [   43     0     4 12361     1    78     0     0     0     1]\n",
      " [  297     0    95   168 12854  1878  2736     3     4   637]\n",
      " [   18     0     0     1     2   104    12     2     0     2]\n",
      " [  119     0     4     0  5205   256 13578     0     0    14]\n",
      " [   10     0    16     0     0     0     2   367     0     7]\n",
      " [   15     0     0     0     0     0    11     3  5564     0]\n",
      " [  303     0     8     2    68     4     2     3     0  1846]]\n",
      "Validation Loss: 389.62129147478845 Validation Accuracy: 0.8467351580944578 Validation Multiclass Weighted F1: 0.8568010633468014 Validation Multiclass Macro F1: 0.7831872620092286\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[81849    41  2316   490   891  5224  1973  1608   117   632]\n",
      " [   20  8610     5     0     2    33     0    13     0     2]\n",
      " [  442     4 11898    31   139    32    60     9     0     4]\n",
      " [  132     1    18 30844     2   313     3     0     0    13]\n",
      " [  339     2   749    85 41116   307   959    54    10  1170]\n",
      " [   25     0     6     4     4   607     4     4     0     7]\n",
      " [  332     0   250     7   883   596 41506   125    90   500]\n",
      " [   16     0     2     0     4    20     4  1826     1     2]\n",
      " [    0     0     0     0     0     1    31    11 14367     0]\n",
      " [   35     0     8     3    30    86    11     8     0  5335]]\n",
      "Epoch: 6 Train Loss: 360.2367263380438 Train Accuracy: 0.9176477847234809 Train Multiclass Weighted F1: 0.9289821785122931 Train Multiclass Macro F1: 0.8232011440142827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:34<00:39, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[24693    35   128    38    55  1559   397   150    80    45]\n",
      " [   11  2560     0     1     0     8     2     2     0     1]\n",
      " [  284     0  3679     0   294     5    25     0     0     1]\n",
      " [   99     4     4 12302     0    79     0     0     0     0]\n",
      " [   24     0    98   168 12527  5092   359     0     2   402]\n",
      " [    7     0     0     1     1   118    13     1     0     0]\n",
      " [ 2591     0     4     0  2687   176 13683     0     0    35]\n",
      " [   39     8     0     0     0     1     2   352     0     0]\n",
      " [   18     0     0     0     0     0    62     1  5512     0]\n",
      " [  455     0     6    11    57     3     0     3     0  1701]]\n",
      "Validation Loss: 500.9046512860805 Validation Accuracy: 0.8314593417492265 Validation Multiclass Weighted F1: 0.8587695510377109 Validation Multiclass Macro F1: 0.7860670112784321\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[82131    76  2340   354   897  5028  1878  1706   113   618]\n",
      " [   12  8610     8     0     0    42     0    11     0     2]\n",
      " [  420     0 11955    17   133    26    58     5     0     5]\n",
      " [  136     3    11 30771     0   399     0     0     0     6]\n",
      " [  302     2   725    86 41326   280   878    37    13  1142]\n",
      " [   30     1     4     7     2   595    11     5     1     5]\n",
      " [  365     1   253     7   868   584 41449    95   125   542]\n",
      " [   13     2     1     0     4    21     5  1828     0     1]\n",
      " [    3     0     0     0     1     2    38     4 14362     0]\n",
      " [   35     0     9     2    35    74    19     8     0  5334]]\n",
      "Epoch: 7 Train Loss: 354.49212749674916 Train Accuracy: 0.9192018911508486 Train Multiclass Weighted F1: 0.9303788536843357 Train Multiclass Macro F1: 0.8234199384214212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:47<00:26, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[23196     7   159    57   696  1396  1236   302    94    37]\n",
      " [    0  2560     1     1     0    18     2     2     0     1]\n",
      " [  253     0  3702     0   304     2    26     0     0     1]\n",
      " [   21     0     4 12357     0   104     0     0     0     2]\n",
      " [   97     0    95   168 13848  1856  1984     1     1   622]\n",
      " [    3     0     0     1     4   115    13     2     0     3]\n",
      " [    8     0     3     0  5366    12 13785     0     0     2]\n",
      " [    0     0     0     0    14     4     2   382     0     0]\n",
      " [   17     0     0     0     0     0    72     1  5503     0]\n",
      " [   16     0     1    11   381     2     2    19     0  1804]]\n",
      "Validation Loss: 418.2168078743125 Validation Accuracy: 0.8328068908269639 Validation Multiclass Weighted F1: 0.8492184289715499 Validation Multiclass Macro F1: 0.7754650499667142\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[83245    54  2412   509   822  4033  1751  1598   110   607]\n",
      " [   13  8609     5     0     1    47     1     7     0     2]\n",
      " [  443     1 11904    35   136    23    62     7     0     8]\n",
      " [  160     1    21 30744     0   384     0     1     0    15]\n",
      " [  307     0   724    90 41483   280   800    37    16  1054]\n",
      " [   18     0     4     5     6   604     3     8     0    13]\n",
      " [  292     0   280     7   777   468 41857    95    96   417]\n",
      " [    9     1     1     0     3    27     8  1824     1     1]\n",
      " [    2     0     1     0     5     1    22     7 14372     0]\n",
      " [   31     0     6    14    29   117    21     4     0  5294]]\n",
      "Epoch: 8 Train Loss: 367.29467497207224 Train Accuracy: 0.925275632151107 Train Multiclass Weighted F1: 0.9345793150092553 Train Multiclass Macro F1: 0.829463285750119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:00<00:12, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[25349    93   136    55    73   468   544   334    88    40]\n",
      " [    5  2570     1     1     0     3     0     4     0     1]\n",
      " [  286     0  3690     0   251    19    41     0     0     1]\n",
      " [  103     0     2 12302     0    73     6     0     0     2]\n",
      " [  314     0   811   168 12490  2699  1490    32     1   667]\n",
      " [   15     0     1     1     6   101     8     7     0     2]\n",
      " [   57     0    16     0  4943   112 14047     0     0     1]\n",
      " [   14     4     0     0     1     9     2   372     0     0]\n",
      " [   14     0     0     0     0     0    65    13  5501     0]\n",
      " [   25     0     7     2    63     3     7     8     0  2121]]\n",
      "Validation Loss: 426.2092094120453 Validation Accuracy: 0.846724377701836 Validation Multiclass Weighted F1: 0.8603761827958402 Validation Multiclass Macro F1: 0.772579820937524\n",
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[82084    93  2802   399   955  4774  1775  1571   103   585]\n",
      " [   15  8610     9     0     2    42     0     5     0     2]\n",
      " [  420     4 11931    13   127    31    71    13     0     9]\n",
      " [  137     0    18 30787     0   370     2     0     0    12]\n",
      " [  341     0   839    83 41158   248  1032    49    13  1028]\n",
      " [   23     1     6     4     6   601     5     5     0    10]\n",
      " [  374     8   249     5   962   559 41625    77   103   327]\n",
      " [   12     0     4     0     5    25     3  1824     0     2]\n",
      " [    1     0     0     4     0     2    35     5 14363     0]\n",
      " [   29     0     7     5    33    72    17    11     0  5342]]\n",
      "Epoch: 9 Train Loss: 357.65341196767986 Train Accuracy: 0.9190630627851284 Train Multiclass Weighted F1: 0.9295380833737055 Train Multiclass Macro F1: 0.8259104377295111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:12<00:00, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BENIGN', 'backdoor', 'ddos', 'dos', 'injection', 'mitm', 'password', 'ransomware', 'scanning', 'xss'])\n",
      "[[23905    16   272    29   669  1644   266   213   134    32]\n",
      " [    2  2560    10     1     0    10     0     0     2     0]\n",
      " [  239     0  3614     0   316     5   114     0     0     0]\n",
      " [   15     0     1 12117     1   349     0     0     0     5]\n",
      " [  347     0    94    85 11820  4574  1203    58     1   490]\n",
      " [    0     0     0     0     6   119     9     7     0     0]\n",
      " [  972     0     1     0   874    27 17231     0    64     7]\n",
      " [    1     0     4     0     0    14     2   381     0     0]\n",
      " [   17     0     0     0     0     0    63     1  5512     0]\n",
      " [  264     0     0     2    40   553     1     8     0  1368]]\n",
      "Validation Loss: 709.8308229913237 Validation Accuracy: 0.8476299306820755 Validation Multiclass Weighted F1: 0.8764538220000032 Validation Multiclass Macro F1: 0.7763328629386927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_processor = data_handling.DataPreprocessor(INGESTED_DATA_PATH, UTILS_PATH)\n",
    "graph_builder = data_handling.GraphBuilder()\n",
    "\n",
    "attack_mapping = data_processor.load_attack_mapping(dataset_name)\n",
    "\n",
    "train_raw, val_raw, test_raw = data_processor.load_mixed_train(dataset_name), data_processor.load_mixed_val(dataset_name), data_processor.load_mixed_test(dataset_name)\n",
    "\n",
    "\n",
    "(train_attrs, train_labels), (val_attrs, val_labels), (test_attrs, test_labels) = data_processor.preprocess_NF(dataset_name, train_raw, keep_IPs_and_timestamp=False, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True), \\\n",
    "            data_processor.preprocess_NF(dataset_name, val_raw, keep_IPs_and_timestamp=False, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True), \\\n",
    "            data_processor.preprocess_NF(dataset_name, test_raw, keep_IPs_and_timestamp=False, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True)\n",
    "\n",
    "# Setup optimizer\n",
    "model = gnn_architectures.MLP(train_attrs.shape[1], classifier_hidden_channels, len(attack_mapping), classifier_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Make sure attrs in alphabetical order and then in tensor form\n",
    "train_attrs = train_attrs[train_attrs.columns.sort_values()]\n",
    "val_attrs = val_attrs[val_attrs.columns.sort_values()]\n",
    "test_attrs = test_attrs[test_attrs.columns.sort_values()]\n",
    "\n",
    "train_attrs = torch.tensor(train_attrs.values, dtype=torch.float32)\n",
    "train_labels = train_labels.to_numpy()\n",
    "labels_torch = torch.Tensor([attack_mapping[attack] for attack in train_labels])\n",
    "\n",
    "val_attrs = torch.tensor(val_attrs.values, dtype=torch.float32)\n",
    "val_labels = val_labels.to_numpy()\n",
    "labels_torch_val = torch.Tensor([attack_mapping[attack] for attack in val_labels])\n",
    "\n",
    "test_attrs = torch.tensor(test_attrs.values, dtype=torch.float32)\n",
    "test_labels = test_labels.to_numpy()\n",
    "labels_torch_test = torch.Tensor([attack_mapping[attack] for attack in test_labels])\n",
    "\n",
    "tensor_dataset = torch.utils.data.TensorDataset(train_attrs.float(), labels_torch.float())\n",
    "tensor_dataset_val = torch.utils.data.TensorDataset(val_attrs.float(), labels_torch_val.float())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(tensor_dataset_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Setup Loss Criterion\n",
    "if weighted_loss:\n",
    "    num_classes = len(attack_mapping)\n",
    "    class_counts = np.zeros(num_classes)\n",
    "    for batch in train_loader:\n",
    "        target = batch[1]\n",
    "        class_counts += target.sum(dim=0).numpy()\n",
    "\n",
    "    # print(f'class counts in training data: {attack_mapping.keys()}:{class_counts}')\n",
    "\n",
    "    total_samples = class_counts.sum()\n",
    "    class_weights = total_samples / (num_classes * class_counts)\n",
    "\n",
    "    weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "best_model_weights = None\n",
    "best_model_val_f1 = 0\n",
    "best_model_weights_epoch = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_weighted_f1 = []\n",
    "val_weighted_f1 = []\n",
    "train_macro_f1 = []\n",
    "val_macro_f1 = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    total_train_loss = 0\n",
    "    epoch_preds = np.array([])\n",
    "    epoch_targets = np.array([])\n",
    "    for batch in train_loader:\n",
    "        train_data, train_targets = batch\n",
    "        train_data, train_targets = train_data.to(device), train_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        batch_preds = model(train_data)\n",
    "        batch_loss = criterion(batch_preds, train_targets)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += batch_loss.item()\n",
    "        epoch_preds = np.concatenate((epoch_preds, batch_preds.argmax(dim=1).cpu().detach().numpy()))\n",
    "        epoch_targets = np.concatenate((epoch_targets, train_targets.argmax(dim=1).cpu().detach().numpy()))\n",
    "\n",
    "    # Calculate metrics\n",
    "    epoch_train_accuracy, epoch_train_f1_weighted, epoch_train_f1_macro = calculate_multiclass_metrics(epoch_preds, epoch_targets, attack_mapping)\n",
    "    train_losses.append(total_train_loss)\n",
    "    train_weighted_f1.append(epoch_train_f1_weighted)\n",
    "    train_macro_f1.append(epoch_train_f1_macro)\n",
    "    print('Epoch:', epoch, 'Train Loss:', total_train_loss, 'Train Accuracy:', epoch_train_accuracy.item(), 'Train Multiclass Weighted F1:', epoch_train_f1_weighted.item(), 'Train Multiclass Macro F1:', epoch_train_f1_macro.item())\n",
    "\n",
    "    total_val_loss = 0\n",
    "    epoch_preds = np.array([])\n",
    "    epoch_targets = np.array([])\n",
    "    for batch in val_loader:\n",
    "        val_data, val_targets = batch\n",
    "        val_data, val_targets = val_data.to(device), val_targets.to(device)\n",
    "        batch_preds = model(val_data)\n",
    "        batch_loss = criterion(batch_preds, val_targets)\n",
    "        total_val_loss += batch_loss.item()\n",
    "        epoch_preds = np.concatenate((epoch_preds, batch_preds.argmax(dim=1).cpu().detach().numpy()))\n",
    "        epoch_targets = np.concatenate((epoch_targets, val_targets.argmax(dim=1).cpu().detach().numpy()))\n",
    "\n",
    "    val_accuracy, val_f1_weighted, val_f1_macro = calculate_multiclass_metrics(epoch_preds, epoch_targets, attack_mapping)\n",
    "    val_losses.append(total_val_loss)\n",
    "    val_weighted_f1.append(val_f1_weighted)\n",
    "    val_macro_f1.append(val_f1_macro)\n",
    "    if val_f1_macro > best_model_val_f1:\n",
    "        best_model_val_f1 = val_f1_macro\n",
    "        best_model_weights = model.state_dict()\n",
    "        best_model_weights_epoch = epoch\n",
    "    print('Validation Loss:', total_val_loss, 'Validation Accuracy:', val_accuracy.item(), 'Validation Multiclass Weighted F1:', val_f1_weighted.item(), 'Validation Multiclass Macro F1:', val_f1_macro.item())\n",
    "\n",
    "os.makedirs(os.path.join(SAVED_MODELS_PATH, dataset_name, 'baselines'), exist_ok=True)\n",
    "experiment_idx = len(os.listdir(os.path.join(SAVED_MODELS_PATH, dataset_name, 'baselines')))\n",
    "experiment_dir = os.path.join(SAVED_MODELS_PATH, dataset_name, 'baselines', f'experiment_{experiment_idx}')\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "experiment_dict = {'dataset': dataset_name, 'gnn_type': 'MLP', 'gnn_layers': 0, 'gnn_hidden_channels': 0, 'classifier_layers': classifier_layers, 'classifier_hidden_channels': classifier_hidden_channels, 'batch_size': batch_size, 'num_epochs': num_epochs, 'weighted_loss': weighted_loss, 'truncate': True}\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_model_weights, os.path.join(experiment_dir, f'best_model_weights_epoch_{best_model_weights_epoch}.pth'))\n",
    "\n",
    "# Save the experiment metadata\n",
    "experiments_results = {'train_losses': train_losses, 'val_losses': val_losses, 'train_weighted_f1': train_weighted_f1, 'val_weighted_f1': val_weighted_f1, 'train_macro_f1': train_macro_f1, 'val_macro_f1': val_macro_f1}\n",
    "with open(os.path.join(experiment_dir, 'results.json'), 'w') as f:\n",
    "    json.dump(experiments_results, f)\n",
    "with open(os.path.join(experiment_dir, 'experiment_metadata.json'), 'w') as f:\n",
    "    json.dump(experiment_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All Model Predictions On Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading in mixed test set...\n",
      "-- Min-Max scaling numerical columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [00:12<00:00, 41.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading in mixed test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\Documenten\\02-work\\01-BNN-UPC\\PPT_GNN_github_version\\data_handling\\data_preprocessing.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ohe[f'{attribute}_{value}'] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Min-Max scaling numerical columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 644/644 [00:19<00:00, 33.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading in mixed test set...\n",
      "-- Min-Max scaling numerical columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 644/644 [00:19<00:00, 33.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading in mixed test set...\n",
      "-- Min-Max scaling numerical columns...\n"
     ]
    }
   ],
   "source": [
    "dataset_to_evaluate = 'NF_ToN_IoT' # Choose from 'NF_ToN_IoT', 'NF_BoT_IoT', 'NF_UNSW_NB15\n",
    "model_type_to_evaluate = 'temporal' # Choose from 'static', 'temporal'\n",
    "weights_to_select = 'best_macro' # Choose from 'best_macro', 'best_weighted', or give a checkpoint number\n",
    "\n",
    "# Set the experiment directories\n",
    "gnn_from_scratch_experiment_dir = os.path.join(SAVED_MODELS_PATH, dataset_to_evaluate, 'experiments', model_type_to_evaluate)\n",
    "gnn_finetune_experiment_dir = os.path.join(SAVED_MODELS_PATH, dataset_to_evaluate, 'fine_tuned_experiments', model_type_to_evaluate)\n",
    "baseline_experiment_dir = os.path.join(SAVED_MODELS_PATH, dataset_to_evaluate, 'baselines')\n",
    "\n",
    "# Check if the experiment directories exist\n",
    "os.makedirs(gnn_from_scratch_experiment_dir, exist_ok=True)\n",
    "os.makedirs(gnn_finetune_experiment_dir, exist_ok=True)\n",
    "os.makedirs(baseline_experiment_dir, exist_ok=True)\n",
    "\n",
    "## 1) Testing routine for all GNNs from scratch ----------\n",
    "\n",
    "for experiment_path in os.listdir(gnn_from_scratch_experiment_dir):\n",
    "    # Check which experiments have already been tested. Skip if test results already exist\n",
    "    experiment_path = os.path.join(gnn_from_scratch_experiment_dir, experiment_path)\n",
    "    files_in_experiment = os.listdir(experiment_path)\n",
    "    if 'test_set_results.pkl' in files_in_experiment:\n",
    "        print(f'Skipping {experiment_path} as test results already exist')\n",
    "        continue\n",
    "    if len(files_in_experiment) == 0:\n",
    "        print(f'Skipping {experiment_path} as no files in experiment')\n",
    "        continue\n",
    "\n",
    "    # Get Experiment Metadata\n",
    "    with open(os.path.join(experiment_path, 'experiment_metadata.json'), 'r') as f:\n",
    "        experiment_dict = json.load(f)\n",
    "\n",
    "    # Get data according to the experiment\n",
    "    test_data = data_processor.load_mixed_test(experiment_dict['dataset'])\n",
    "    attack_mapping = data_processor.load_attack_mapping(experiment_dict['dataset'])\n",
    "    if 'NF' in experiment_dict['dataset']:\n",
    "        test_attrs, test_labels = data_processor.preprocess_NF(experiment_dict['dataset'], test_data, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=experiment_dict['truncate'])\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset name')\n",
    "\n",
    "    # Get the graph list\n",
    "    test_windows = graph_builder.time_window_with_flow_duration(test_attrs, experiment_dict['window_size'], experiment_dict['window_stride'])\n",
    "    temporal = False\n",
    "    features = test_attrs.columns\n",
    "    features = [feat for feat in features if feat not in ['Dst IP', 'Dst Port', 'Flow Duration Graph Building', 'Src IP', 'Src Port', 'Timestamp']]\n",
    "    if model_type_to_evaluate == 'temporal':\n",
    "        temporal = True\n",
    "        test_graphs, test_window_indices_for_classification = graph_builder.build_spatio_temporal_pyg_graphs(test_windows, test_attrs, test_labels, experiment_dict['window_memory'], experiment_dict['flow_memory'], experiment_dict['include_port'], features, attack_mapping)\n",
    "    elif model_type_to_evaluate == 'static':\n",
    "        test_graphs = graph_builder.build_static_pyg_graphs(test_windows, test_attrs, test_labels, experiment_dict['include_port'], features, attack_mapping)\n",
    "    if experiment_dict['self_loops']:\n",
    "        test_graphs = [AddSelfLoops()(graph) for graph in test_graphs]\n",
    "    sample_graph = test_graphs[0]\n",
    "\n",
    "    # Load the model\n",
    "    if model_type_to_evaluate == 'temporal':\n",
    "        gnn_base = gnn_architectures.TemporalPlusConv_v2(sample_graph.metadata(), experiment_dict['gnn_hidden_channels'], experiment_dict['gnn_layers'])\n",
    "    elif model_type_to_evaluate == 'static':\n",
    "        gnn_base = gnn_architectures.SAGE(sample_graph.metadata(), experiment_dict['gnn_hidden_channels'], experiment_dict['gnn_layers'])\n",
    "    else:\n",
    "        raise ValueError('Unknown GNN type')\n",
    "\n",
    "    model = gnn_architectures.multiclass_NIDS_model(gnn_base, len(attack_mapping), experiment_dict['classifier_hidden_channels'], experiment_dict['classifier_layers'], temporal)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # lazy init\n",
    "    with torch.no_grad():\n",
    "        _ = model(test_graphs[0].x_dict, test_graphs[0].edge_index_dict)\n",
    "\n",
    "    # Load the best model weights\n",
    "    if weights_to_select == 'best_macro':\n",
    "        best_model_weights = [f for f in os.listdir(experiment_path) if 'macro' in f][0]\n",
    "    elif weights_to_select == 'best_weighted':\n",
    "        best_model_weights = [f for f in os.listdir(experiment_path) if 'weighted' in f][0]\n",
    "    else:\n",
    "        best_model_weights = f'model_weights_checkpoint_epoch_{weights_to_select}.pth'\n",
    "    best_model_weights = torch.load(os.path.join(experiment_path, best_model_weights), map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    model.to(device)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loader = DataLoader(test_graphs, batch_size=experiment_dict['batch_size'], shuffle=False)\n",
    "    model.eval()\n",
    "    test_preds = np.array([])\n",
    "    test_targets = np.array([])\n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        test_data = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(test_data.x_dict, test_data.edge_index_dict)\n",
    "            preds = torch.argmax(out, dim=1)\n",
    "\n",
    "            if idx == 0:\n",
    "                test_probs = out.cpu().numpy()\n",
    "            else:\n",
    "                test_probs = np.concatenate((test_probs, out.cpu().numpy()), axis=0)\n",
    "            test_preds = np.concatenate((test_preds, preds.cpu().numpy()), axis=0)\n",
    "            test_targets = np.concatenate((test_targets, torch.argmax(test_data['con'].y, dim = 1).cpu().numpy()), axis=0)\n",
    "\n",
    "    # Deduplicate the results if temporal model (cause in temporal model, we have reoccurig flows connected to different windows)\n",
    "    if experiment_dict['gnn_type'] == 'temporal':\n",
    "        test_preds, test_targets, test_probs = deduplicate_multiclass_sliding_window_results(test_preds, test_targets, test_probs, test_window_indices_for_classification)\n",
    "\n",
    "    # Save test_preds, test_targets and test_probs in pickle file\n",
    "    with open(os.path.join(experiment_path, 'test_set_results.pkl'), 'wb') as f:\n",
    "        pickle.dump({'test_preds': test_preds, 'test_targets': test_targets, 'test_probs': test_probs}, f)\n",
    "\n",
    "## 2) Testing routine for all baselines --------\n",
    "\n",
    "# Change gnn_fine_tune_experiment_dir list to include all experiments including the k-shot ones that have subdirs\n",
    "experiments_dirs_in_fine_tune = []\n",
    "files_in_fine_tune = os.listdir(gnn_finetune_experiment_dir)\n",
    "for f in files_in_fine_tune:\n",
    "    if 'experiment_metadata.json' in os.listdir(os.path.join(gnn_finetune_experiment_dir, f)):\n",
    "        experiments_dirs_in_fine_tune.append(os.path.join(gnn_finetune_experiment_dir, f))\n",
    "        continue\n",
    "    subdirs = os.listdir(os.path.join(gnn_finetune_experiment_dir, f))\n",
    "    for subdir in subdirs:\n",
    "        if 'experiment_metadata.json' in os.listdir(os.path.join(gnn_finetune_experiment_dir, f, subdir)):\n",
    "            experiments_dirs_in_fine_tune.append(os.path.join(gnn_finetune_experiment_dir, f, subdir))\n",
    "\n",
    "# NOw evaluate the fine-tuned models\n",
    "for experiment_path in experiments_dirs_in_fine_tune:\n",
    "    flow_memory = 20\n",
    "    files_in_experiment = os.listdir(experiment_path)\n",
    "    # Check if already a fine-tuned model (having metadata) or a k-shot learning directory having more submodules\n",
    "    if 'experiment_metadata.json' in files_in_experiment:\n",
    "        if 'test_set_results.pkl' in files_in_experiment:\n",
    "            print(f'Skipping {experiment_path} as test results already exist')\n",
    "            continue\n",
    "        if len(files_in_experiment) == 0:\n",
    "            print(f'Skipping {experiment_path} as no files in experiment')\n",
    "            continue\n",
    "\n",
    "    # Get Experiment Metadata\n",
    "    with open(os.path.join(experiment_path, 'experiment_metadata.json'), 'r') as f:\n",
    "        experiment_dict = json.load(f)\n",
    "\n",
    "    # Get data according to the experiment\n",
    "    test_data = data_processor.load_mixed_test(experiment_dict['dataset'])\n",
    "    attack_mapping = data_processor.load_attack_mapping(experiment_dict['dataset'])\n",
    "\n",
    "    cross_data_preprocessing = False\n",
    "    if 'pretrain_strategy' in experiment_dict.keys():\n",
    "        if experiment_dict[\"pretrain_strategy\"] != 'in_context':\n",
    "            cross_data_preprocessing = True\n",
    "\n",
    "    if cross_data_preprocessing:\n",
    "        test_attrs, test_labels = data_processor.preprocess_NF('all', test_data, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True)\n",
    "    else:\n",
    "        test_attrs, test_labels = data_processor.preprocess_NF(dataset_to_evaluate, test_data, keep_IPs_and_timestamp=True, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True)\n",
    "\n",
    "    # Get the graph list\n",
    "    test_windows = graph_builder.time_window_with_flow_duration(test_attrs, experiment_dict['window_size'], experiment_dict['window_stride'])\n",
    "    features = test_attrs.columns\n",
    "    features = [feat for feat in features if feat not in ['Dst IP', 'Dst Port', 'Flow Duration Graph Building', 'Src IP', 'Src Port', 'Timestamp']]\n",
    "    if model_type_to_evaluate == 'temporal':\n",
    "        temporal = True\n",
    "        test_graphs, test_window_indices_for_classification = graph_builder.build_spatio_temporal_pyg_graphs(test_windows, test_attrs, test_labels, experiment_dict['window_memory'], flow_memory, False, features, attack_mapping, True)\n",
    "    elif model_type_to_evaluate == 'static':\n",
    "        temporal = False\n",
    "        test_graphs = graph_builder.build_static_pyg_graphs(test_windows, test_attrs, test_labels, experiment_dict['include_port'], features, attack_mapping)\n",
    "    if experiment_dict['self_loops']:\n",
    "        test_graphs = [AddSelfLoops()(graph) for graph in test_graphs]\n",
    "    sample_graph = test_graphs[0]\n",
    "\n",
    "    # Load the model\n",
    "    if model_type_to_evaluate == 'temporal':\n",
    "        gnn_base = gnn_architectures.TemporalPlusConv_v2(sample_graph.metadata(), experiment_dict['gnn_hidden_channels'], experiment_dict['gnn_layers'])\n",
    "    elif model_type_to_evaluate == 'static':\n",
    "        gnn_base = gnn_architectures.SAGE(sample_graph.metadata(), experiment_dict['gnn_hidden_channels'], experiment_dict['gnn_layers'])\n",
    "    else:\n",
    "        raise ValueError('Unknown GNN type')\n",
    "\n",
    "    model = gnn_architectures.multiclass_NIDS_model(gnn_base, len(attack_mapping), experiment_dict['classifier_hidden_channels'], experiment_dict['classifier_layers'], temporal)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # lazy init\n",
    "    with torch.no_grad():\n",
    "        _ = model(test_graphs[0].x_dict, test_graphs[0].edge_index_dict)\n",
    "\n",
    "    # Load the best model weights\n",
    "    best_model_weights = [f for f in os.listdir(experiment_path) if 'best_model_weights' in f][0]\n",
    "    best_model_weights = torch.load(os.path.join(experiment_path, best_model_weights), map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    model.to(device)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loader = DataLoader(test_graphs, batch_size=experiment_dict['batch_size'])\n",
    "    model.eval()\n",
    "    test_preds = np.array([])\n",
    "    test_targets = np.array([])\n",
    "    for idx,batch in enumerate(test_loader):\n",
    "        test_data = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(test_data.x_dict, test_data.edge_index_dict)\n",
    "            preds = torch.argmax(out, dim=1)\n",
    "\n",
    "            if idx == 0:\n",
    "                test_probs = out.cpu().numpy()\n",
    "            else:\n",
    "                test_probs = np.concatenate((test_probs, out.cpu().numpy()), axis=0)\n",
    "\n",
    "            test_preds = np.concatenate((test_preds, preds.cpu().numpy()), axis=0)\n",
    "            test_targets = np.concatenate((test_targets, torch.argmax(test_data['con'].y, dim = 1).cpu().numpy()), axis=0)\n",
    "\n",
    "    if experiment_dict['gnn_type'] == 'static':\n",
    "        test_preds, test_targets, test_probs = deduplicate_multiclass_sliding_window_results(test_preds, test_targets, test_probs, test_window_indices_for_classification)\n",
    "\n",
    "    # Save test_preds, test_targets and test_probs in pickle file\n",
    "    with open(os.path.join(experiment_path, 'test_set_results.pkl'), 'wb') as f:\n",
    "        pickle.dump({'test_preds': test_preds, 'test_targets': test_targets, 'test_probs': test_probs}, f)\n",
    "\n",
    "## 3) Testing routine for all baselines --------\n",
    "\n",
    "for experiment_path in os.listdir(baseline_experiment_dir):\n",
    "    experiment_path = os.path.join(baseline_experiment_dir, experiment_path)\n",
    "    files_in_experiment = os.listdir(experiment_path)\n",
    "    if 'test_set_results.pkl' in files_in_experiment:\n",
    "        print(f'Skipping {experiment_path} as test results already exist')\n",
    "        continue\n",
    "    if len(files_in_experiment) == 0:\n",
    "        print(f'Skipping {experiment_path} as no files in experiment')\n",
    "        continue\n",
    "\n",
    "    # Get Experiment Metadata\n",
    "    with open(os.path.join(experiment_path, 'experiment_metadata.json'), 'r') as f:\n",
    "        experiment_dict = json.load(f)\n",
    "\n",
    "    if experiment_dict['gnn_type'] == 'MLP':\n",
    "\n",
    "        # Get data according to the experiment\n",
    "        test_data = data_processor.load_mixed_test(experiment_dict['dataset'])\n",
    "        attack_mapping = data_processor.load_attack_mapping(experiment_dict['dataset'])\n",
    "        test_attrs, test_labels = data_processor.preprocess_NF(experiment_dict['dataset'], test_data, keep_IPs_and_timestamp=False, binary=False, remove_minority_labels=False, only_attacks=False, scale=True, truncate=True)\n",
    "        \n",
    "        # Setup model\n",
    "        model = gnn_architectures.MLP(test_attrs.shape[1], experiment_dict['classifier_hidden_channels'], len(attack_mapping), experiment_dict['classifier_layers'])\n",
    "\n",
    "        # Load the best model weights\n",
    "        best_model_weights = [f for f in os.listdir(experiment_path) if 'best_model_weights' in f][0]\n",
    "        best_model_weights = torch.load(os.path.join(experiment_path, best_model_weights), map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(best_model_weights)\n",
    "\n",
    "        # Make sure attrs in alphabetical order and then in tensor form\n",
    "        test_attrs = test_attrs[test_attrs.columns.sort_values()]\n",
    "        test_attrs = torch.tensor(test_attrs.values, dtype=torch.float32)\n",
    "        test_labels = test_labels.to_numpy()\n",
    "        labels_torch_test = torch.Tensor([attack_mapping[attack] for attack in test_labels])\n",
    "        tensor_dataset = torch.utils.data.TensorDataset(test_attrs.float(), labels_torch_test.float())\n",
    "        test_loader = torch.utils.data.DataLoader(tensor_dataset, batch_size=experiment_dict['batch_size'], shuffle=False)\n",
    "\n",
    "        # Evaluate the model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        test_preds_list = np.array([])\n",
    "        test_targets_list = np.array([])\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            test_data, test_targets = batch\n",
    "            test_data, test_targets = test_data.to(device), test_targets.to(device)\n",
    "            with torch.no_grad():\n",
    "                out = torch.nn.functional.softmax(model(test_data), dim=1)\n",
    "                preds = torch.argmax(out, dim=1).cpu().numpy()\n",
    "                targets = torch.argmax(test_targets, dim = 1).cpu().numpy()\n",
    "\n",
    "                if idx == 0:\n",
    "                    test_probs_list = out.cpu().numpy()\n",
    "                else:\n",
    "                    test_probs_list = np.concatenate((test_probs_list, out.cpu().numpy()), axis=0)\n",
    "\n",
    "                test_preds_list = np.concatenate((test_preds_list, preds), axis=0)\n",
    "                test_targets_list = np.concatenate((test_targets_list, targets), axis=0)\n",
    "\n",
    "\n",
    "        # Save test_preds, test_targets and test_probs in pickle file\n",
    "        with open(os.path.join(experiment_path, 'test_set_results.pkl'), 'wb') as f:\n",
    "            pickle.dump({'test_preds': test_preds_list, 'test_targets': test_targets_list, 'test_probs': test_probs_list}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Test Set Predictions to Evaluation Metrics across All Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN from scratch results\n",
      "Empty DataFrame\n",
      "Columns: [dataset, model_type, window_size, window_memory, multiclass_acc, multiclass_f1_weighted, multiclass_f1_macro, multiclass_roc_auc_macro_ovr, multiclass_roc_auc_macro_ovo, multiclass_roc_auc_weighted_ovr, multiclass_roc_auc_weighted_ovo, binary_macro_f1, binary_weighted_f1]\n",
      "Index: []\n",
      "Baselines results\n",
      "      dataset model_type  multiclass_acc  multiclass_f1_weighted  \\\n",
      "0  NF_ToN_IoT        MLP        0.863127                0.885071   \n",
      "\n",
      "   multiclass_f1_macro  multiclass_roc_auc_macro_ovr  \\\n",
      "0             0.756166                       0.96405   \n",
      "\n",
      "   multiclass_roc_auc_macro_ovo  multiclass_roc_auc_weighted_ovr  \\\n",
      "0                      0.970319                         0.967359   \n",
      "\n",
      "   multiclass_roc_auc_weighted_ovo  binary_macro_f1  binary_weighted_f1  \n",
      "0                         0.968857         0.906766            0.970484  \n",
      "Fine-tuned GNN results\n",
      "Empty DataFrame\n",
      "Columns: [dataset, model_type, window_size, window_memory, multiclass_acc, multiclass_f1_weighted, multiclass_f1_macro, multiclass_roc_auc_macro_ovr, multiclass_roc_auc_macro_ovo, multiclass_roc_auc_weighted_ovr, multiclass_roc_auc_weighted_ovo, binary_macro_f1, binary_weighted_f1]\n",
      "Index: []\n",
      "K-Shot Learning results\n",
      "      dataset model_type  k_shot_frac pretrain_strategy window_size  \\\n",
      "0  NF_ToN_IoT   temporal     0.049974       out_context           5   \n",
      "1  NF_ToN_IoT   temporal     0.049974    no_pretraining           5   \n",
      "\n",
      "  window_memory  multiclass_acc  multiclass_f1_weighted  multiclass_f1_macro  \\\n",
      "0             5        0.187390                0.169936             0.127225   \n",
      "1             5        0.267875                0.227564             0.169471   \n",
      "\n",
      "   multiclass_roc_auc_macro_ovr  multiclass_roc_auc_macro_ovo  \\\n",
      "0                      0.867278                      0.872359   \n",
      "1                      0.866671                      0.872625   \n",
      "\n",
      "   multiclass_roc_auc_weighted_ovr  multiclass_roc_auc_weighted_ovo  \\\n",
      "0                         0.828243                         0.850175   \n",
      "1                         0.839486                         0.854138   \n",
      "\n",
      "   binary_macro_f1  binary_weighted_f1  best_train_macro_f1  \\\n",
      "0         0.611543            0.762905             0.095100   \n",
      "1         0.672326            0.750665             0.142371   \n",
      "\n",
      "   best_train_weighted_f1  best_val_macro_f1  best_val_weighted_f1  \n",
      "0                0.216609           0.086926              0.149607  \n",
      "1                0.248400           0.180851              0.271668  \n"
     ]
    }
   ],
   "source": [
    "# Load pickle file \n",
    "datasets_to_evaluate = ['NF_ToN_IoT'] # Choose from 'NF_ToN_IoT', 'NF_BoT_IoT', 'NF_UNSW_NB15\n",
    "model_types_to_evaluate = ['temporal'] # Choose from 'temporal' and 'static'\n",
    "weights_to_select = 'best_macro' # Choose from 'best_macro', 'best_weighted', or give a checkpoint number\n",
    "\n",
    "gnn_from_scratch_df = pd.DataFrame(columns=['dataset', 'model_type', 'window_size', 'window_memory', 'multiclass_acc', 'multiclass_f1_weighted', 'multiclass_f1_macro', 'multiclass_roc_auc_macro_ovr', 'multiclass_roc_auc_macro_ovo', 'multiclass_roc_auc_weighted_ovr', 'multiclass_roc_auc_weighted_ovo', 'binary_macro_f1', 'binary_weighted_f1'])\n",
    "baselines_df = pd.DataFrame(columns=['dataset', 'model_type', 'multiclass_acc', 'multiclass_f1_weighted', 'multiclass_f1_macro', 'multiclass_roc_auc_macro_ovr', 'multiclass_roc_auc_macro_ovo', 'multiclass_roc_auc_weighted_ovr', 'multiclass_roc_auc_weighted_ovo', 'binary_macro_f1', 'binary_weighted_f1'])\n",
    "fine_tuned_gnn_df = pd.DataFrame(columns=['dataset', 'model_type', 'window_size', 'window_memory', 'multiclass_acc', 'multiclass_f1_weighted', 'multiclass_f1_macro', 'multiclass_roc_auc_macro_ovr', 'multiclass_roc_auc_macro_ovo', 'multiclass_roc_auc_weighted_ovr', 'multiclass_roc_auc_weighted_ovo', 'binary_macro_f1', 'binary_weighted_f1'])\n",
    "k_shot_learning_df = pd.DataFrame(columns=['dataset', 'model_type', 'k_shot_frac','pretrain_strategy', 'window_size', 'window_memory','multiclass_acc', 'multiclass_f1_weighted', 'multiclass_f1_macro', 'multiclass_roc_auc_macro_ovr', 'multiclass_roc_auc_macro_ovo', 'multiclass_roc_auc_weighted_ovr', 'multiclass_roc_auc_weighted_ovo', 'binary_macro_f1', 'binary_weighted_f1', 'best_train_macro_f1', 'best_train_weighted_f1', 'best_val_macro_f1', 'best_val_weighted_f1'])\n",
    "\n",
    "for dataset_name in datasets_to_evaluate:\n",
    "    \n",
    "    for model_type_to_evaluate in model_types_to_evaluate:\n",
    "        # Set the experiment directories\n",
    "        gnn_from_scratch_experiment_dir = os.path.join(SAVED_MODELS_PATH, dataset_name, 'experiments', model_type_to_evaluate)\n",
    "        gnn_finetune_experiment_dir = os.path.join(SAVED_MODELS_PATH, dataset_name, 'fine_tuned_experiments', model_type_to_evaluate)\n",
    "        baseline_experiment_dir = os.path.join(SAVED_MODELS_PATH, dataset_name, 'baselines')\n",
    "\n",
    "        # Check if the experiment directories exist\n",
    "        os.makedirs(gnn_from_scratch_experiment_dir, exist_ok=True)\n",
    "        os.makedirs(gnn_finetune_experiment_dir, exist_ok=True)\n",
    "        os.makedirs(baseline_experiment_dir, exist_ok=True)\n",
    "\n",
    "        for experiment_path in os.listdir(gnn_from_scratch_experiment_dir):\n",
    "            experiment_path = os.path.join(gnn_from_scratch_experiment_dir, experiment_path)\n",
    "            with open(f'{experiment_path}/experiment_metadata.json', 'r') as f:\n",
    "                experiment_dict = json.load(f)\n",
    "\n",
    "            if 'test_set_results.pkl' not in os.listdir(experiment_path):\n",
    "                print(f'Skipping {experiment_path} with window size {experiment_dict[\"window_size\"]} and window memory {experiment_dict[\"window_memory\"]} as test results do not exist')\n",
    "                continue\n",
    "            \n",
    "            with open(f'{experiment_path}/test_set_results.pkl', 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "\n",
    "\n",
    "            gnn_type, window_size, window_memory = experiment_dict['gnn_type'], experiment_dict['window_size'], experiment_dict['window_memory']\n",
    "            if window_memory == 5:\n",
    "                multiclass_acc, multiclass_f1_weighted, multiclass_f1_macro, multiclass_roc_auc_macro_ovr, multiclass_roc_auc_macro_ovo, multiclass_roc_auc_weighted_ovr, multiclass_roc_auc_weighted_ovo, binary_macro_f1, binary_weighted_f1 = calculate_multiclass_test_metrics(results['test_preds'], results['test_targets'], results['test_probs'])\n",
    "                gnn_from_scratch_df = gnn_from_scratch_df.append({'dataset':dataset_name ,'model_type': gnn_type, 'window_size': window_size, 'window_memory': window_memory, 'multiclass_acc': multiclass_acc, 'multiclass_f1_weighted': multiclass_f1_weighted, 'multiclass_f1_macro': multiclass_f1_macro, 'multiclass_roc_auc_macro_ovr': multiclass_roc_auc_macro_ovr, 'multiclass_roc_auc_macro_ovo': multiclass_roc_auc_macro_ovo, 'multiclass_roc_auc_weighted_ovr': multiclass_roc_auc_weighted_ovr, 'multiclass_roc_auc_weighted_ovo': multiclass_roc_auc_weighted_ovo, 'binary_macro_f1': binary_macro_f1, 'binary_weighted_f1': binary_weighted_f1}, ignore_index=True)\n",
    "\n",
    "        for experiment_path in os.listdir(baseline_experiment_dir):\n",
    "\n",
    "            experiment_path = os.path.join(baseline_experiment_dir, experiment_path)\n",
    "            with open(f'{experiment_path}/experiment_metadata.json', 'r') as f:\n",
    "                experiment_dict = json.load(f)\n",
    "\n",
    "            if 'test_set_results.pkl' not in os.listdir(experiment_path):\n",
    "                print(f'Skipping {experiment_path} as test results do not exist')\n",
    "                continue\n",
    "            \n",
    "            with open(f'{experiment_path}/test_set_results.pkl', 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "\n",
    "            multiclass_acc, multiclass_f1_weighted, multiclass_f1_macro, multiclass_roc_auc_macro_ovr, multiclass_roc_auc_macro_ovo, multiclass_roc_auc_weighted_ovr, multiclass_roc_auc_weighted_ovo, binary_macro_f1, binary_weighted_f1 = calculate_multiclass_test_metrics(results['test_preds'], results['test_targets'], results['test_probs'])\n",
    "            baselines_df = baselines_df.append({'dataset':dataset_name ,'model_type': experiment_dict['gnn_type'], 'multiclass_acc': multiclass_acc, 'multiclass_f1_weighted': multiclass_f1_weighted, 'multiclass_f1_macro': multiclass_f1_macro, 'multiclass_roc_auc_macro_ovr': multiclass_roc_auc_macro_ovr, 'multiclass_roc_auc_macro_ovo': multiclass_roc_auc_macro_ovo, 'multiclass_roc_auc_weighted_ovr': multiclass_roc_auc_weighted_ovr, 'multiclass_roc_auc_weighted_ovo': multiclass_roc_auc_weighted_ovo, 'binary_macro_f1': binary_macro_f1, 'binary_weighted_f1': binary_weighted_f1}, ignore_index=True)\n",
    "        \n",
    "        # Change gnn_fine_tune_experiment_dir list to include all experiments including the k-shot ones that have subdirs\n",
    "        # Change gnn_fine_tune_experiment_dir list to include all experiments including the k-shot ones that have subdirs\n",
    "        experiments_dirs_in_fine_tune = []\n",
    "        files_in_fine_tune = os.listdir(gnn_finetune_experiment_dir)\n",
    "        for f in files_in_fine_tune:\n",
    "            if 'experiment_metadata.json' in os.listdir(os.path.join(gnn_finetune_experiment_dir, f)):\n",
    "                experiments_dirs_in_fine_tune.append(os.path.join(gnn_finetune_experiment_dir, f))\n",
    "                continue\n",
    "            subdirs = os.listdir(os.path.join(gnn_finetune_experiment_dir, f))\n",
    "            for subdir in subdirs:\n",
    "                if 'experiment_metadata.json' in os.listdir(os.path.join(gnn_finetune_experiment_dir, f, subdir)):\n",
    "                    experiments_dirs_in_fine_tune.append(os.path.join(gnn_finetune_experiment_dir, f, subdir))\n",
    "        for experiment_path in experiments_dirs_in_fine_tune:\n",
    "        \n",
    "            with open(f'{experiment_path}/experiment_metadata.json', 'r') as f:\n",
    "                experiment_dict = json.load(f)\n",
    "\n",
    "            k_shot_model = False\n",
    "            if 'pretrain_strategy' in experiment_dict.keys():\n",
    "                k_shot_model = True\n",
    "                with open(f'{experiment_path}/results.json', 'r') as f:\n",
    "                    train_results = json.load(f)\n",
    "\n",
    "            if 'test_set_results.pkl' not in os.listdir(experiment_path):\n",
    "                print(f'Skipping {experiment_path} with window size {experiment_dict[\"window_size\"]} and window memory {experiment_dict[\"window_memory\"]} as test results do not exist')\n",
    "                continue\n",
    "            \n",
    "            with open(f'{experiment_path}/test_set_results.pkl', 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "\n",
    "            gnn_type, window_size, window_memory = experiment_dict['gnn_type'], experiment_dict['window_size'], experiment_dict['window_memory']\n",
    "\n",
    "            if k_shot_model:\n",
    "                k_shot_frac = experiment_dict['K-shot-dataset_frac']\n",
    "                pretrain_strategy = experiment_dict['pretrain_strategy']\n",
    "                multiclass_acc, multiclass_f1_weighted, multiclass_f1_macro, multiclass_roc_auc_macro_ovr, multiclass_roc_auc_macro_ovo, multiclass_roc_auc_weighted_ovr, multiclass_roc_auc_weighted_ovo, binary_macro_f1, binary_weighted_f1 = calculate_multiclass_test_metrics(results['test_preds'], results['test_targets'], results['test_probs'])\n",
    "                best_train_macro_f1, best_val_macro_f1 = max(train_results['train_macro_f1']), max(train_results['val_macro_f1'])\n",
    "                best_train_weighted_f1, best_val_weighted_f1 = max(train_results['train_weighted_f1']), max(train_results['val_weighted_f1'])\n",
    "\n",
    "                k_shot_learning_df = k_shot_learning_df.append({'dataset':dataset_name ,'model_type': gnn_type, 'k_shot_frac': k_shot_frac, 'pretrain_strategy': pretrain_strategy, 'window_size': window_size, 'window_memory': window_memory, 'multiclass_acc': multiclass_acc, 'multiclass_f1_weighted': multiclass_f1_weighted, 'multiclass_f1_macro': multiclass_f1_macro, 'multiclass_roc_auc_macro_ovr': multiclass_roc_auc_macro_ovr, 'multiclass_roc_auc_macro_ovo': multiclass_roc_auc_macro_ovo, 'multiclass_roc_auc_weighted_ovr': multiclass_roc_auc_weighted_ovr, 'multiclass_roc_auc_weighted_ovo': multiclass_roc_auc_weighted_ovo, 'binary_macro_f1': binary_macro_f1, 'binary_weighted_f1': binary_weighted_f1, 'best_train_macro_f1': best_train_macro_f1, 'best_val_macro_f1': best_val_macro_f1, 'best_train_weighted_f1': best_train_weighted_f1, 'best_val_weighted_f1': best_val_weighted_f1}, ignore_index=True)\n",
    "            \n",
    "            else:\n",
    "                multiclass_acc, multiclass_f1_weighted, multiclass_f1_macro, multiclass_roc_auc_macro_ovr, multiclass_roc_auc_macro_ovo, multiclass_roc_auc_weighted_ovr, multiclass_roc_auc_weighted_ovo, binary_macro_f1, binary_weighted_f1 = calculate_multiclass_test_metrics(results['test_preds'], results['test_targets'], results['test_probs'])\n",
    "\n",
    "                fine_tuned_gnn_df = fine_tuned_gnn_df.append({'dataset':dataset_name ,'model_type': gnn_type, 'window_size': window_size, 'window_memory': window_memory, 'multiclass_acc': multiclass_acc, 'multiclass_f1_weighted': multiclass_f1_weighted, 'multiclass_f1_macro': multiclass_f1_macro, 'multiclass_roc_auc_macro_ovr': multiclass_roc_auc_macro_ovr, 'multiclass_roc_auc_macro_ovo': multiclass_roc_auc_macro_ovo, 'multiclass_roc_auc_weighted_ovr': multiclass_roc_auc_weighted_ovr, 'multiclass_roc_auc_weighted_ovo': multiclass_roc_auc_weighted_ovo, 'binary_macro_f1': binary_macro_f1, 'binary_weighted_f1': binary_weighted_f1}, ignore_index=True)\n",
    "\n",
    "print('GNN from scratch results')\n",
    "print(gnn_from_scratch_df)  \n",
    "print('Baselines results')\n",
    "print(baselines_df)\n",
    "print('Fine-tuned GNN results')\n",
    "print(fine_tuned_gnn_df)\n",
    "print('K-Shot Learning results')\n",
    "print(k_shot_learning_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
